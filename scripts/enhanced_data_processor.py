import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import warnings
import os
import re
from typing import Dict, List, Optional, Tuple
warnings.filterwarnings('ignore')

class EnhancedTikTokDataProcessor:
    """å¢å¼ºç‰ˆ TikTok æ•°æ®å¤„ç†ç±» - æ”¯æŒåŸºç¡€ä¿¡æ¯å±•ç¤ºå’Œç‚¹å‡»é‡åˆ†æ"""
    
    def __init__(self, 
                 redash_data_dir: str = 'data/redash_data',
                 accounts_file_path: str = 'data/postingManager_data/accounts_detail.xlsx',
                 clicks_data_dir: str = 'data/clicks',
                 accounts_df: pd.DataFrame = None,
                 redash_df: pd.DataFrame = None,
                 clicks_df: pd.DataFrame = None):
        """
        åˆå§‹åŒ–å¢å¼ºç‰ˆæ•°æ®å¤„ç†å™¨
        
        Args:
            redash_data_dir (str): redash æ•°æ®ç›®å½•è·¯å¾„
            accounts_file_path (str): accounts detail æ•°æ®æ–‡ä»¶è·¯å¾„
            clicks_data_dir (str): clicks æ•°æ®ç›®å½•è·¯å¾„
        """
        self.redash_data_dir = redash_data_dir
        self.accounts_file_path = accounts_file_path
        self.clicks_data_dir = clicks_data_dir
        
        # æ•°æ®å­˜å‚¨
        self.merged_df = None
        self.group_mapping = None
        self.clicks_df = clicks_df
        self.accounts_df = accounts_df
        self.redash_df = redash_df
        
        # é“¾æ¥åˆ°åˆ†ç»„çš„æ˜ å°„
        self.link_group_mapping = {
            'https://insnap.ai/videos': 'yujie_main_avatar',
            'https://insnap.ai/zh/download': 'wan_produce101'
        }
        
    def load_latest_redash_data(self) -> Optional[pd.DataFrame]:
        """åŠ è½½æœ€æ–°çš„ redash æ•°æ®æ–‡ä»¶"""
        try:
            if not os.path.exists(self.redash_data_dir):
                print(f"âŒ Redash æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.redash_data_dir}")
                return None
                
            # è·å–æœ€æ–°çš„ redash æ•°æ®æ–‡ä»¶
            redash_files = [f for f in os.listdir(self.redash_data_dir) 
                          if f.startswith('redash_data_') and f.endswith('.csv')]
            
            if not redash_files:
                print(f"âŒ æœªæ‰¾åˆ° redash æ•°æ®æ–‡ä»¶åœ¨: {self.redash_data_dir}")
                return None
                
            latest_file = max(redash_files, key=lambda x: os.path.getctime(os.path.join(self.redash_data_dir, x)))
            file_path = os.path.join(self.redash_data_dir, latest_file)
            
            print(f"æ­£åœ¨åŠ è½½ redash æ•°æ®: {latest_file}")
            redash_df = pd.read_csv(file_path, low_memory=False)
            print("Redash columns:", redash_df.columns.tolist())
            
            # æ•°æ®é¢„å¤„ç†
            if 'YMDdate' in redash_df.columns:
                redash_df['date'] = pd.to_datetime(redash_df['YMDdate'], errors='coerce')
            elif 'date' in redash_df.columns:
                redash_df['date'] = pd.to_datetime(redash_df['date'], errors='coerce')
            
            redash_df = redash_df.dropna(subset=['date'])
            
            # ç¡®ä¿ user_id ä¸ºå­—ç¬¦ä¸²ç±»å‹
            redash_df['user_id'] = redash_df['user_id'].astype(str)
            
            # ç¡®ä¿æ•°å€¼åˆ—ä¸ºæ•°å€¼ç±»å‹
            numeric_columns = [
                'view_count', 'like_count', 'comment_count', 'share_count', 
                'post_count', 'view_per_post', 'like_per_post', 'comment_per_post', 
                'share_per_post', 'view_diff', 'like_diff', 'comment_diff', 
                'share_diff', 'post_diff'
            ]
            
            for col in numeric_columns:
                if col in redash_df.columns:
                    redash_df[col] = pd.to_numeric(redash_df[col], errors='coerce').fillna(0)
            
            print(f"âœ… Redash æ•°æ®åŠ è½½æˆåŠŸ: {redash_df.shape}")
            return redash_df
            
        except Exception as e:
            print(f"âŒ Redash æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            return None
    
    def load_accounts_data(self) -> Optional[pd.DataFrame]:
        """åŠ è½½ accounts detail æ•°æ®"""
        try:
            print("æ­£åœ¨åŠ è½½ accounts detail æ•°æ®...")
            if not os.path.exists(self.accounts_file_path):
                print(f"âŒ Accounts æ–‡ä»¶ä¸å­˜åœ¨: {self.accounts_file_path}")
                return None
                
            accounts_df = pd.read_excel(self.accounts_file_path)
            self.accounts_df = accounts_df
            
            # åˆ›å»º group æ˜ å°„
            group_mapping = accounts_df[['Tiktok ID', 'Groups']].drop_duplicates()
            group_mapping = group_mapping.rename(columns={'Tiktok ID': 'user_id', 'Groups': 'group'})
            
            # ç¡®ä¿ user_id ä¸ºå­—ç¬¦ä¸²ç±»å‹
            group_mapping['user_id'] = group_mapping['user_id'].astype(str)
            
            # å¤„ç†ç©ºå€¼
            group_mapping['group'] = group_mapping['group'].fillna('Unknown')
            
            print(f"âœ… Accounts æ•°æ®åŠ è½½æˆåŠŸ: {accounts_df.shape}")
            print(f"âœ… Group æ˜ å°„åˆ›å»ºæˆåŠŸ: {group_mapping.shape}")
            return group_mapping
            
        except Exception as e:
            print(f"âŒ Accounts æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            return None
    
    def load_clicks_data(self) -> Optional[pd.DataFrame]:
        """åŠ è½½ç‚¹å‡»æ•°æ®"""
        try:
            if not os.path.exists(self.clicks_data_dir):
                print(f"âŒ Clicks æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.clicks_data_dir}")
                return None
                
            # è·å–æœ€æ–°çš„ clicks æ•°æ®æ–‡ä»¶
            clicks_files = [f for f in os.listdir(self.clicks_data_dir) 
                          if f.endswith('.csv')]
            
            if not clicks_files:
                print(f"âŒ æœªæ‰¾åˆ° clicks æ•°æ®æ–‡ä»¶åœ¨: {self.clicks_data_dir}")
                return None
                
            latest_file = max(clicks_files, key=lambda x: os.path.getctime(os.path.join(self.clicks_data_dir, x)))
            file_path = os.path.join(self.clicks_data_dir, latest_file)
            
            print(f"æ­£åœ¨åŠ è½½ clicks æ•°æ®: {latest_file}")
            clicks_df = pd.read_csv(file_path, low_memory=False)
            
            # æ•°æ®é¢„å¤„ç†
            if 'timestamp' in clicks_df.columns:
                clicks_df['date'] = pd.to_datetime(clicks_df['timestamp']).dt.date
            elif 'date' in clicks_df.columns:
                clicks_df['date'] = pd.to_datetime(clicks_df['date']).dt.date
            
            # æå–é¡µé¢ç±»å‹å’Œé“¾æ¥ä¿¡æ¯
            clicks_df['page_type'] = clicks_df.get('page_type', 'unknown')
            clicks_df['page_url'] = clicks_df.get('page_url', '')
            
            print(f"âœ… Clicks æ•°æ®åŠ è½½æˆåŠŸ: {clicks_df.shape}")
            return clicks_df
            
        except Exception as e:
            print(f"âŒ Clicks æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            return None
    
    def merge_data(self) -> bool:
        """åˆå¹¶æ‰€æœ‰æ•°æ®"""
        try:
            print("æ­£åœ¨åˆå¹¶æ•°æ®...")
            import streamlit as st
            st.write("[DEBUG] æ­£åœ¨åˆå¹¶æ•°æ®...")
            
            # ç¡®ä¿åœ¨äº‘ç«¯ä¹Ÿèƒ½çœ‹åˆ°è°ƒè¯•ä¿¡æ¯
            import sys
            st.write(f"[DEBUG] Pythonè·¯å¾„: {sys.path[:3]}")  # æ˜¾ç¤ºå‰3ä¸ªè·¯å¾„
            
            # ä¼˜å…ˆç”¨ä¼ å…¥çš„ DataFrame
            redash_df = self.redash_df if self.redash_df is not None else self.load_latest_redash_data()
            print(f"[DEBUG] redash_df shape: {redash_df.shape if redash_df is not None else 'None'}")
            st.write(f"[DEBUG] redash_df shape: {redash_df.shape if redash_df is not None else 'None'}")
            
            group_mapping = None
            if self.accounts_df is not None:
                print(f"[DEBUG] ä½¿ç”¨ä¼ å…¥çš„ accounts_df, shape: {self.accounts_df.shape}")
                st.write(f"[DEBUG] ä½¿ç”¨ä¼ å…¥çš„ accounts_df, shape: {self.accounts_df.shape}")
                accounts_df = self.accounts_df
                print(f"[DEBUG] accounts_df columns: {accounts_df.columns.tolist()}")
                st.write(f"[DEBUG] accounts_df columns: {accounts_df.columns.tolist()}")
                # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
                if 'Tiktok ID' not in accounts_df.columns or 'Groups' not in accounts_df.columns:
                    print(f"[DEBUG] accounts_df columns: {accounts_df.columns.tolist()}")
                    st.error(f"[DEBUG] accounts_df columns: {accounts_df.columns.tolist()}")
                    raise ValueError("accounts_df ç¼ºå°‘å¿…è¦çš„åˆ—: 'Tiktok ID' æˆ– 'Groups'")
                # å¼ºåˆ¶ç±»å‹è½¬æ¢
                accounts_df['Tiktok ID'] = accounts_df['Tiktok ID'].astype(str)
                print(f"[DEBUG] accounts_df['Tiktok ID'] dtype: {accounts_df['Tiktok ID'].dtype}")
                st.write(f"[DEBUG] accounts_df['Tiktok ID'] dtype: {accounts_df['Tiktok ID'].dtype}")
                print(f"[DEBUG] accounts_df['Tiktok ID'] sample: {accounts_df['Tiktok ID'].unique()[:5]}")
                st.write(f"[DEBUG] accounts_df['Tiktok ID'] sample: {accounts_df['Tiktok ID'].unique()[:5]}")
                group_mapping = accounts_df[['Tiktok ID', 'Groups']].drop_duplicates()
                group_mapping = group_mapping.rename(columns={'Tiktok ID': 'user_id', 'Groups': 'group'})
                group_mapping['user_id'] = group_mapping['user_id'].astype(str)
                group_mapping['group'] = group_mapping['group'].fillna('Unknown')
                print(f"[DEBUG] group_mapping shape: {group_mapping.shape}")
                st.write(f"[DEBUG] group_mapping shape: {group_mapping.shape}")
            else:
                group_mapping = self.load_accounts_data()
                print(f"[DEBUG] ä»æ–‡ä»¶åŠ è½½ group_mapping, shape: {group_mapping.shape if group_mapping is not None else 'None'}")
                st.write(f"[DEBUG] ä»æ–‡ä»¶åŠ è½½ group_mapping, shape: {group_mapping.shape if group_mapping is not None else 'None'}")
            
            clicks_df = self.clicks_df if self.clicks_df is not None else self.load_clicks_data()
            print(f"[DEBUG] clicks_df shape: {clicks_df.shape if clicks_df is not None else 'None'}")
            
            # è‡ªåŠ¨è¡¥é½ clicks_df çš„ date å­—æ®µ
            if clicks_df is not None and 'date' not in clicks_df.columns:
                if 'timestamp' in clicks_df.columns:
                    clicks_df['date'] = pd.to_datetime(clicks_df['timestamp']).dt.date
                else:
                    raise KeyError("clicks_df ç¼ºå°‘ 'date' æˆ– 'timestamp' å­—æ®µ")
            
            if redash_df is None:
                print("[DEBUG] redash_df ä¸º Noneï¼Œåˆå¹¶å¤±è´¥")
                st.error("[DEBUG] redash_df ä¸º Noneï¼Œåˆå¹¶å¤±è´¥")
                self.merged_df = None
                return False
                
            if group_mapping is None:
                print("[DEBUG] group_mapping ä¸º Noneï¼Œåˆå¹¶å¤±è´¥")
                st.error("[DEBUG] group_mapping ä¸º Noneï¼Œåˆå¹¶å¤±è´¥")
                self.merged_df = None
                return False
            
            print(f"[DEBUG] å¼€å§‹åˆå¹¶ï¼Œredash_df columns: {redash_df.columns.tolist()}")
            st.write(f"[DEBUG] å¼€å§‹åˆå¹¶ï¼Œredash_df columns: {redash_df.columns.tolist()}")
            print(f"[DEBUG] group_mapping columns: {group_mapping.columns.tolist()}")
            st.write(f"[DEBUG] group_mapping columns: {group_mapping.columns.tolist()}")
            
            # ç¡®ä¿ redash_df æœ‰æ­£ç¡®çš„ date åˆ—
            if 'date' not in redash_df.columns:
                print("[DEBUG] redash_df ç¼ºå°‘ 'date' åˆ—ï¼Œå°è¯•ä»å…¶ä»–åˆ—åˆ›å»º...")
                st.write("[DEBUG] redash_df ç¼ºå°‘ 'date' åˆ—ï¼Œå°è¯•ä»å…¶ä»–åˆ—åˆ›å»º...")
                if 'YMDdate' in redash_df.columns:
                    redash_df['date'] = pd.to_datetime(redash_df['YMDdate'], errors='coerce')
                    print("[DEBUG] ä» 'YMDdate' åˆ—åˆ›å»º 'date' åˆ—")
                    st.write("[DEBUG] ä» 'YMDdate' åˆ—åˆ›å»º 'date' åˆ—")
                else:
                    print("[DEBUG] redash_df ç¼ºå°‘æ—¥æœŸåˆ—ï¼Œå®é™…åˆ—: ", redash_df.columns.tolist())
                    st.error("[DEBUG] redash_df ç¼ºå°‘æ—¥æœŸåˆ—ï¼Œå®é™…åˆ—: " + str(redash_df.columns.tolist()))
                    raise ValueError("redash_df ç¼ºå°‘æ—¥æœŸåˆ—")
            
            # ç¡®ä¿ date åˆ—æ˜¯ datetime ç±»å‹
            if not pd.api.types.is_datetime64_any_dtype(redash_df['date']):
                redash_df['date'] = pd.to_datetime(redash_df['date'], errors='coerce')
            
            # ç§»é™¤æ— æ•ˆçš„æ—¥æœŸè¡Œ
            redash_df = redash_df.dropna(subset=['date'])
            print(f"[DEBUG] å¤„ç†æ—¥æœŸå redash_df shape: {redash_df.shape}")
            st.write(f"[DEBUG] å¤„ç†æ—¥æœŸå redash_df shape: {redash_df.shape}")
            
            # å¼ºåˆ¶ç±»å‹è½¬æ¢
            if 'user_id' in redash_df.columns:
                redash_df['user_id'] = redash_df['user_id'].astype(str)
                print(f"[DEBUG] redash_df['user_id'] dtype: {redash_df['user_id'].dtype}")
                st.write(f"[DEBUG] redash_df['user_id'] dtype: {redash_df['user_id'].dtype}")
                print(f"[DEBUG] redash_df['user_id'] sample: {redash_df['user_id'].unique()[:5]}")
                st.write(f"[DEBUG] redash_df['user_id'] sample: {redash_df['user_id'].unique()[:5]}")
            else:
                print("[DEBUG] redash_df ç¼ºå°‘ 'user_id' åˆ—ï¼Œå®é™…åˆ—: ", redash_df.columns.tolist())
                st.error("[DEBUG] redash_df ç¼ºå°‘ 'user_id' åˆ—ï¼Œå®é™…åˆ—: " + str(redash_df.columns.tolist()))
                raise ValueError("redash_df ç¼ºå°‘ 'user_id' åˆ—")
            
            # åˆå¹¶ redash å’Œ accounts æ•°æ®
            st.write("[DEBUG] å¼€å§‹æ‰§è¡Œ merge æ“ä½œ...")
            merged_df = redash_df.merge(group_mapping, on='user_id', how='left')
            print(f"[DEBUG] åˆå¹¶å shape: {merged_df.shape}")
            st.write(f"[DEBUG] åˆå¹¶å shape: {merged_df.shape}")
            print(f"[DEBUG] merged_df['group'] value_counts: {merged_df['group'].value_counts(dropna=False) if 'group' in merged_df.columns else 'æ— groupåˆ—'}")
            st.write(f"[DEBUG] merged_df['group'] value_counts: {merged_df['group'].value_counts(dropna=False) if 'group' in merged_df.columns else 'æ— groupåˆ—'}")
            print(f"[DEBUG] merged_df head:\n{merged_df.head()}")
            st.write(f"[DEBUG] merged_df head:")
            st.dataframe(merged_df.head())
            
            # å¼ºåˆ¶ç»Ÿä¸€åˆ†ç»„å­—æ®µåä¸º group
            if 'Groups' in merged_df.columns:
                merged_df = merged_df.rename(columns={'Groups': 'group'})
            merged_df['group'] = merged_df['group'].fillna('Unknown')

            self.merged_df = merged_df
            self.group_mapping = group_mapping
            self.clicks_df = clicks_df
            
            print(f"âœ… æ•°æ®åˆå¹¶æˆåŠŸ: {merged_df.shape}")
            st.success(f"âœ… æ•°æ®åˆå¹¶æˆåŠŸ: {merged_df.shape}")
            return True
            
        except Exception as e:
            print(f"âŒ æ•°æ®åˆå¹¶å¤±è´¥: {str(e)}")
            st.error(f"âŒ æ•°æ®åˆå¹¶å¤±è´¥: {str(e)}")
            self.merged_df = None
            return False
    
    def get_available_groups(self) -> List[str]:
        """è·å–æ‰€æœ‰å¯ç”¨çš„åˆ†ç»„ï¼ˆæ”¯æŒæ¨¡ç³ŠåŒ¹é…ï¼‰"""
        if self.merged_df is None:
            return []
        
        all_groups_raw = self.merged_df['group'].dropna().unique().tolist()
        split_groups = set()
        
        for g in all_groups_raw:
            if isinstance(g, str):
                for part in re.split(r'[\,\|/;ï¼Œï¼›]', g):
                    part = part.strip()
                    if part:
                        split_groups.add(part)
        
        return sorted(list(split_groups))
    
    def filter_data_by_groups(self, df: pd.DataFrame, selected_groups: List[str]) -> pd.DataFrame:
        """
        æ ¹æ®åˆ†ç»„å…³é”®è¯ç­›é€‰å¹¶å½’ä¸€åŒ–åˆ†ç»„æ ‡ç­¾ï¼ˆåªè¦åŒ…å«å…³é”®è¯å°±å½’ä¸ºè¯¥å…³é”®è¯ï¼‰
        """
        if not selected_groups:
            return df
        # å½’ä¸€åŒ–åˆ†ç»„ï¼šåªè¦ group å­—æ®µåŒ…å«å…³é”®è¯ï¼Œå°±å½’ä¸ºè¯¥å…³é”®è¯
        def normalize_group(row):
            for sg in selected_groups:
                if sg in str(row['group']):
                    return sg
            return row['group']
        filtered_df = df[df['group'].apply(lambda x: any(sg in str(x) for sg in selected_groups))].copy()
        filtered_df['group'] = filtered_df.apply(normalize_group, axis=1)
        return filtered_df
    
    def get_daily_metrics(self, start_date: Optional[str] = None, 
                         end_date: Optional[str] = None,
                         groups: Optional[List[str]] = None) -> pd.DataFrame:
        """
        è·å–æ¯æ—¥æŒ‡æ ‡æ•°æ®
        
        Args:
            start_date (str): å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date (str): ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            groups (list): åˆ†ç»„åˆ—è¡¨
        
        Returns:
            pd.DataFrame: æ¯æ—¥æŒ‡æ ‡æ•°æ®
        """
        if self.merged_df is None:
            return pd.DataFrame()
        
        filtered_df = self.merged_df.copy()
        
        # æ—¥æœŸç­›é€‰
        if start_date:
            start_date = pd.Timestamp(start_date)
            filtered_df = filtered_df[filtered_df['date'] >= start_date]
        
        if end_date:
            end_date = pd.Timestamp(end_date)
            filtered_df = filtered_df[filtered_df['date'] <= end_date]
        
        # åˆ†ç»„ç­›é€‰
        if groups:
            filtered_df = self.filter_data_by_groups(filtered_df, groups)
        
        # æ¯æ—¥èšåˆ
        metrics = ['view_count', 'like_count', 'comment_count', 'share_count', 'post_count']
        daily_data = filtered_df.groupby('date')[metrics].sum().reset_index()
        
        return daily_data
    
    def get_group_daily_metrics(self, start_date: Optional[str] = None,
                               end_date: Optional[str] = None,
                               groups: Optional[List[str]] = None) -> pd.DataFrame:
        """
        è·å–åˆ†ç»„æ¯æ—¥æŒ‡æ ‡æ•°æ®
        
        Args:
            start_date (str): å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date (str): ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            groups (list): åˆ†ç»„åˆ—è¡¨
        
        Returns:
            pd.DataFrame: åˆ†ç»„æ¯æ—¥æŒ‡æ ‡æ•°æ®
        """
        if self.merged_df is None:
            return pd.DataFrame()
        
        filtered_df = self.merged_df.copy()
        
        # æ—¥æœŸç­›é€‰
        if start_date:
            start_date = pd.Timestamp(start_date)
            filtered_df = filtered_df[filtered_df['date'] >= start_date]
        
        if end_date:
            end_date = pd.Timestamp(end_date)
            filtered_df = filtered_df[filtered_df['date'] <= end_date]
        
        # åˆ†ç»„ç­›é€‰
        if groups:
            filtered_df = self.filter_data_by_groups(filtered_df, groups)
        
        # åˆ†ç»„æ¯æ—¥èšåˆ
        metrics = ['view_count', 'like_count', 'comment_count', 'share_count', 'post_count']
        group_daily_data = filtered_df.groupby(['date', 'group'])[metrics].sum().reset_index()
        
        return group_daily_data
    
    def get_clicks_metrics(self, start_date: Optional[str] = None,
                          end_date: Optional[str] = None) -> Dict:
        """
        è·å–ç‚¹å‡»é‡æŒ‡æ ‡
        
        Args:
            start_date (str): å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date (str): ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
        
        Returns:
            Dict: ç‚¹å‡»é‡æŒ‡æ ‡æ•°æ®
        """
        if self.clicks_df is None:
            return {}
        
        filtered_clicks = self.clicks_df.copy()
        print("[DEBUG] filtered_clicks columns:", filtered_clicks.columns)
        print("[DEBUG] filtered_clicks head:", filtered_clicks.head())
        print("[DEBUG] filtered_clicks shape:", filtered_clicks.shape)
        if filtered_clicks.empty:
            print("[DEBUG] filtered_clicks is empty")
            return {}
        if 'date' not in filtered_clicks.columns:
            if 'timestamp' in filtered_clicks.columns:
                filtered_clicks['date'] = pd.to_datetime(filtered_clicks['timestamp']).dt.date
            else:
                print("[DEBUG] filtered_clicks columns:", filtered_clicks.columns)
                raise KeyError("clicks æ•°æ®ç¼ºå°‘ 'date' æˆ– 'timestamp' å­—æ®µ")
        # ä¿è¯ date å­—æ®µä¸º pd.Timestamp ç±»å‹
        filtered_clicks['date'] = pd.to_datetime(filtered_clicks['date'])
        # æ—¥æœŸç­›é€‰
        if start_date:
            start_date = pd.to_datetime(start_date)
            filtered_clicks = filtered_clicks[filtered_clicks['date'] >= start_date]
        if end_date:
            end_date = pd.to_datetime(end_date)
            filtered_clicks = filtered_clicks[filtered_clicks['date'] <= end_date]
        
        # æ¯æ—¥ç‚¹å‡»é‡ç»Ÿè®¡
        daily_clicks = filtered_clicks.groupby('date').size().reset_index(name='clicks_count')
        
        # è®¡ç®—å¢é•¿ç‡
        daily_clicks['clicks_growth'] = daily_clicks['clicks_count'].pct_change() * 100
        
        # æŒ‰é¡µé¢ç±»å‹ç»Ÿè®¡
        page_type_clicks = filtered_clicks.groupby('page_type').size().reset_index(name='clicks_count')
        
        # æŒ‰é“¾æ¥ç»Ÿè®¡
        link_clicks = filtered_clicks.groupby('page_url').size().reset_index(name='clicks_count')
        
        return {
            'daily_clicks': daily_clicks,
            'page_type_clicks': page_type_clicks,
            'link_clicks': link_clicks,
            'total_clicks': len(filtered_clicks),
            'unique_dates': filtered_clicks['date'].nunique()
        }
    
    def get_clicks_conversion_analysis(self, start_date: Optional[str] = None,
                                     end_date: Optional[str] = None) -> pd.DataFrame:
        """
        è·å–ç‚¹å‡»è½¬åŒ–åˆ†ææ•°æ®
        
        Args:
            start_date (str): å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date (str): ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
        
        Returns:
            pd.DataFrame: è½¬åŒ–åˆ†ææ•°æ®
        """
        if self.clicks_df is None or self.merged_df is None:
            return pd.DataFrame()
        
        # è·å–ç‚¹å‡»æ•°æ®
        clicks_metrics = self.get_clicks_metrics(start_date, end_date)
        daily_clicks = clicks_metrics.get('daily_clicks', pd.DataFrame())
        
        if daily_clicks.empty:
            return pd.DataFrame()
        
        # è·å– TikTok æ•°æ®
        tiktok_daily = self.get_daily_metrics(start_date, end_date)
        
        if tiktok_daily.empty:
            return pd.DataFrame()
        
        # åˆå¹¶æ•°æ®å‰ï¼Œç¡®ä¿æ—¥æœŸç±»å‹ä¸€è‡´
        if 'date' in daily_clicks.columns:
            daily_clicks['date'] = pd.to_datetime(daily_clicks['date'])
        if 'date' in tiktok_daily.columns:
            tiktok_daily['date'] = pd.to_datetime(tiktok_daily['date'])

        # åˆå¹¶æ•°æ®
        conversion_data = daily_clicks.merge(tiktok_daily, on='date', how='outer')
        conversion_data = conversion_data.fillna(0)
        
        # è®¡ç®—è½¬åŒ–ç‡ï¼ˆé¿å…é™¤é›¶é”™è¯¯ï¼‰
        conversion_data['clicks_to_views_ratio'] = conversion_data.apply(
            lambda row: (row['clicks_count'] / row['view_count'] * 100) if row['view_count'] > 0 else 0, 
            axis=1
        )
        
        conversion_data['clicks_to_likes_ratio'] = conversion_data.apply(
            lambda row: (row['clicks_count'] / row['like_count'] * 100) if row['like_count'] > 0 else 0, 
            axis=1
        )
        
        return conversion_data
    
    def get_group_clicks_analysis(self, start_date: Optional[str] = None,
                                end_date: Optional[str] = None) -> pd.DataFrame:
        """
        è·å–åˆ†ç»„ç‚¹å‡»åˆ†ææ•°æ®
        
        Args:
            start_date (str): å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date (str): ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
        
        Returns:
            pd.DataFrame: åˆ†ç»„ç‚¹å‡»åˆ†ææ•°æ®
        """
        if self.clicks_df is None or self.merged_df is None:
            return pd.DataFrame()
        
        # è·å–åˆ†ç»„ TikTok æ•°æ®
        group_daily = self.get_group_daily_metrics(start_date, end_date)
        
        if group_daily.empty:
            return pd.DataFrame()
        
        # ç¡®ä¿ clicks_df['date'] ä¸º pd.Timestamp ç±»å‹
        self.clicks_df['date'] = pd.to_datetime(self.clicks_df['date'])
        group_daily['date'] = pd.to_datetime(group_daily['date'])
        
        # æŒ‰é“¾æ¥æ˜ å°„åˆ†ç»„
        link_group_data = []
        for link, target_group in self.link_group_mapping.items():
            # ç­›é€‰åŒ…å«ç›®æ ‡åˆ†ç»„çš„æ•°æ®
            group_data = group_daily[group_daily['group'].str.contains(target_group, case=False, na=False)]
            # ç¡®ä¿ group_data['date'] ä¹Ÿæ˜¯ pd.Timestamp ç±»å‹
            group_data['date'] = pd.to_datetime(group_data['date'])
            
            if not group_data.empty:
                # è·å–è¯¥é“¾æ¥çš„ç‚¹å‡»æ•°æ®
                link_clicks = self.clicks_df[
                    (self.clicks_df['page_url'].str.contains(link, case=False, na=False)) &
                    (self.clicks_df['date'] >= group_data['date'].min()) &
                    (self.clicks_df['date'] <= group_data['date'].max())
                ]
                
                if not link_clicks.empty:
                    daily_link_clicks = link_clicks.groupby('date').size().reset_index(name='link_clicks')
                    
                    # åˆå¹¶æ•°æ®
                    merged_data = group_data.merge(daily_link_clicks, on='date', how='left')
                    merged_data['link'] = link
                    merged_data['target_group'] = target_group
                    merged_data['link_clicks'] = merged_data['link_clicks'].fillna(0)
                    
                    link_group_data.append(merged_data)
        
        if link_group_data:
            return pd.concat(link_group_data, ignore_index=True)
        else:
            return pd.DataFrame()
    
    def get_data_summary(self) -> Dict:
        """è·å–æ•°æ®æ‘˜è¦"""
        if self.merged_df is None:
            return {}
        
        try:
            import streamlit as st
            st.write("[DEBUG] å¼€å§‹è·å–æ•°æ®æ‘˜è¦...")
            st.write(f"[DEBUG] merged_df columns: {self.merged_df.columns.tolist()}")
            st.write(f"[DEBUG] merged_df shape: {self.merged_df.shape}")
            
            # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
            if 'date' not in self.merged_df.columns:
                st.error(f"[DEBUG] merged_df ç¼ºå°‘ 'date' åˆ—ï¼Œå®é™…åˆ—: {self.merged_df.columns.tolist()}")
                raise KeyError("merged_df ç¼ºå°‘ 'date' åˆ—")
            
            if 'user_id' not in self.merged_df.columns:
                st.error(f"[DEBUG] merged_df ç¼ºå°‘ 'user_id' åˆ—ï¼Œå®é™…åˆ—: {self.merged_df.columns.tolist()}")
                raise KeyError("merged_df ç¼ºå°‘ 'user_id' åˆ—")
            
            # è®¡ç®—æ€»æµè§ˆé‡
            total_views = self.merged_df['view_count'].sum() if 'view_count' in self.merged_df.columns else 0
            
            # è·å–æ—¥æœŸèŒƒå›´
            date_min = self.merged_df['date'].min()
            date_max = self.merged_df['date'].max()
            st.write(f"[DEBUG] æ—¥æœŸèŒƒå›´: {date_min} åˆ° {date_max}")
            
            summary = {
                'total_records': len(self.merged_df),
                'unique_accounts': self.merged_df['user_id'].nunique(),
                'total_views': total_views,  # æ›¿æ¢åˆ†ç»„æ•°é‡ä¸ºæ€»æµè§ˆé‡
                'date_range': {
                    'start': date_min,
                    'end': date_max
                },
                'matched_records': len(self.merged_df[self.merged_df['group'] != 'Unknown']),
                'unmatched_records': len(self.merged_df[self.merged_df['group'] == 'Unknown']),
                'match_rate': len(self.merged_df[self.merged_df['group'] != 'Unknown']) / len(self.merged_df) * 100
            }
            
            st.write("[DEBUG] æ•°æ®æ‘˜è¦è·å–æˆåŠŸ")
            return summary
            
        except Exception as e:
            st.error(f"[DEBUG] è·å–æ•°æ®æ‘˜è¦å¤±è´¥: {str(e)}")
            print(f"[DEBUG] è·å–æ•°æ®æ‘˜è¦å¤±è´¥: {str(e)}")
            return {}
        
        if self.clicks_df is not None:
            summary['clicks_data'] = {
                'total_clicks': len(self.clicks_df),
                'unique_dates': self.clicks_df['date'].nunique(),
                'date_range': {
                    'start': self.clicks_df['date'].min(),
                    'end': self.clicks_df['date'].max()
                }
            }
        
        # è·å–æ˜¨æ—¥å¯¹æ¯”æ•°æ®
        yesterday_comparison = self.get_yesterday_comparison()
        if yesterday_comparison:
            summary['yesterday_comparison'] = yesterday_comparison
        
        return summary
    
    def get_yesterday_comparison(self) -> Dict:
        """
        è·å–æ˜¨æ—¥å¯¹æ¯”æ•°æ®
        è¿”å›å½“å‰å€¼ä¸æ˜¨æ—¥å€¼çš„å¯¹æ¯”ä¿¡æ¯
        """
        if self.merged_df is None:
            return {}
        
        try:
            # è·å–æœ€æ–°æ—¥æœŸå’Œæ˜¨æ—¥æ—¥æœŸ
            latest_date = self.merged_df['date'].max()
            yesterday_date = latest_date - pd.Timedelta(days=1)
            
            # è·å–æœ€æ–°æ—¥æœŸçš„æ•°æ®
            latest_data = self.merged_df[self.merged_df['date'] == latest_date]
            yesterday_data = self.merged_df[self.merged_df['date'] == yesterday_date]
            
            comparison = {}
            
            # 1. æ€»è®°å½•æ•°å¯¹æ¯”
            current_records = len(latest_data)
            yesterday_records = len(yesterday_data)
            records_diff = current_records - yesterday_records
            records_pct = (records_diff / yesterday_records * 100) if yesterday_records > 0 else 0
            comparison['total_records'] = {
                'current': current_records,
                'yesterday': yesterday_records,
                'diff': records_diff,
                'pct': records_pct
            }
            
            # 2. è´¦å·æ•°é‡å¯¹æ¯”
            current_accounts = latest_data['user_id'].nunique()
            yesterday_accounts = yesterday_data['user_id'].nunique()
            accounts_diff = current_accounts - yesterday_accounts
            accounts_pct = (accounts_diff / yesterday_accounts * 100) if yesterday_accounts > 0 else 0
            comparison['unique_accounts'] = {
                'current': current_accounts,
                'yesterday': yesterday_accounts,
                'diff': accounts_diff,
                'pct': accounts_pct
            }
            
            # 3. æ€»æµè§ˆé‡å¯¹æ¯”
            current_views = latest_data['view_count'].sum() if 'view_count' in latest_data.columns else 0
            yesterday_views = yesterday_data['view_count'].sum() if 'view_count' in yesterday_data.columns else 0
            views_diff = current_views - yesterday_views
            views_pct = (views_diff / yesterday_views * 100) if yesterday_views > 0 else 0
            comparison['total_views'] = {
                'current': current_views,
                'yesterday': yesterday_views,
                'diff': views_diff,
                'pct': views_pct
            }
            
            # 4. æ€»ç‚¹å‡»é‡å¯¹æ¯”
            if self.clicks_df is not None:
                latest_clicks = self.clicks_df.copy()
                if 'timestamp' in latest_clicks.columns:
                    latest_clicks['date'] = pd.to_datetime(latest_clicks['timestamp']).dt.date
                elif 'date' in latest_clicks.columns:
                    latest_clicks['date'] = pd.to_datetime(latest_clicks['date']).dt.date
                
                current_clicks = len(latest_clicks[latest_clicks['date'] == latest_date])
                yesterday_clicks = len(latest_clicks[latest_clicks['date'] == yesterday_date])
                clicks_diff = current_clicks - yesterday_clicks
                clicks_pct = (clicks_diff / yesterday_clicks * 100) if yesterday_clicks > 0 else 0
                comparison['total_clicks'] = {
                    'current': current_clicks,
                    'yesterday': yesterday_clicks,
                    'diff': clicks_diff,
                    'pct': clicks_pct
                }
            
            # 5. æ¯æ—¥å¢é‡æŒ‡æ ‡å¯¹æ¯”
            latest_increments = self.get_latest_day_increment_metrics()
            if not latest_increments.empty:
                latest_row = latest_increments.iloc[-1]  # æœ€æ–°ä¸€å¤©
                yesterday_row = latest_increments.iloc[-2] if len(latest_increments) > 1 else latest_row
                
                increment_metrics = ['view_count_inc', 'post_count_inc', 'like_count_inc', 'comment_count_inc', 'share_count_inc']
                for metric in increment_metrics:
                    if metric in latest_row and metric in yesterday_row:
                        current_val = latest_row[metric]
                        yesterday_val = yesterday_row[metric]
                        diff = current_val - yesterday_val
                        pct = (diff / yesterday_val * 100) if yesterday_val != 0 else 0
                        
                        comparison[f'{metric}_increment'] = {
                            'current': current_val,
                            'yesterday': yesterday_val,
                            'diff': diff,
                            'pct': pct
                        }
            
            return comparison
            
        except Exception as e:
            print(f"âš ï¸ è·å–æ˜¨æ—¥å¯¹æ¯”æ•°æ®å¤±è´¥: {e}")
            return {}
    
    def save_processed_data(self, output_dir: str = 'data/processed'):
        """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
        try:
            os.makedirs(output_dir, exist_ok=True)
            
            if self.merged_df is not None:
                merged_path = os.path.join(output_dir, 'enhanced_merged_tiktok_data.csv')
                self.merged_df.to_csv(merged_path, index=False)
                print(f"âœ… åˆå¹¶æ•°æ®å·²ä¿å­˜: {merged_path}")
            
            if self.clicks_df is not None:
                clicks_path = os.path.join(output_dir, 'processed_clicks_data.csv')
                self.clicks_df.to_csv(clicks_path, index=False)
                print(f"âœ… ç‚¹å‡»æ•°æ®å·²ä¿å­˜: {clicks_path}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ•°æ®ä¿å­˜å¤±è´¥: {str(e)}")
            return False 

    def get_daily_increment_metrics(self, start_date: Optional[str] = None, end_date: Optional[str] = None) -> pd.DataFrame:
        """
        è·å–æ‰€æœ‰è´¦å·æ¯æ—¥æ–°å¢å…³é”®æŒ‡æ ‡ï¼ˆä»¥ user_id ä¸ºå•ä½åš diffï¼Œå†æŒ‰æ—¥æœŸèšåˆï¼‰
        """
        if self.merged_df is None:
            return pd.DataFrame()
        df = self.merged_df.copy()
        df = df.sort_values(['user_id', 'date'])
        for col in ['view_count', 'like_count', 'comment_count', 'share_count', 'post_count']:
            diff = df.groupby('user_id')[col].diff()
            if not isinstance(diff, pd.Series):
                diff = pd.Series([0.0]*len(df), index=df.index)
            else:
                diff = pd.Series(diff.values, index=df.index)
            diff = diff.fillna(0)
            df[f'{col}_inc'] = diff
        if start_date:
            start_date = pd.Timestamp(start_date)
            df = df[df['date'] >= start_date]
        if end_date:
            end_date = pd.Timestamp(end_date)
            df = df[df['date'] <= end_date]
        agg = df.groupby('date')[['view_count_inc', 'like_count_inc', 'comment_count_inc', 'share_count_inc', 'post_count_inc']].sum().reset_index()
        all_dates = pd.date_range(df['date'].min(), df['date'].max())
        agg = agg.set_index('date').reindex(all_dates, fill_value=0).rename_axis('date').reset_index()
        return agg

    def get_group_daily_increment_metrics(self, start_date: Optional[str] = None, end_date: Optional[str] = None, groups: Optional[List[str]] = None) -> pd.DataFrame:
        """
        è·å–åˆ†ç»„æ¯æ—¥æ–°å¢å…³é”®æŒ‡æ ‡ï¼ˆåˆ†ç»„å½’ä¸€åŒ–åï¼Œuser_id+dateåšdiffï¼Œå†æŒ‰date+groupèšåˆï¼‰
        """
        if self.merged_df is None:
            return pd.DataFrame()
        df = self.merged_df.copy()
        if groups:
            df = self.filter_data_by_groups(df, groups)
        df = df.sort_values(['user_id', 'date'])
        for col in ['view_count', 'like_count', 'comment_count', 'share_count', 'post_count']:
            diff = df.groupby('user_id')[col].diff()
            if not isinstance(diff, pd.Series):
                diff = pd.Series([0.0]*len(df), index=df.index)
            else:
                diff = pd.Series(diff.values, index=df.index)
            diff = diff.fillna(0)
            df[f'{col}_inc'] = diff
        if start_date:
            start_date = pd.Timestamp(start_date)
            df = df[df['date'] >= start_date]
        if end_date:
            end_date = pd.Timestamp(end_date)
            df = df[df['date'] <= end_date]
        agg = df.groupby(['date', 'group'])[['view_count_inc', 'like_count_inc', 'comment_count_inc', 'share_count_inc', 'post_count_inc']].sum().reset_index()
        all_dates = pd.date_range(df['date'].min(), df['date'].max())
        all_groups = agg['group'].unique()
        idx = pd.MultiIndex.from_product([all_dates, all_groups], names=['date', 'group'])
        agg = agg.set_index(['date', 'group']).reindex(idx, fill_value=0).reset_index()
        return agg 

    def get_link_conversion_analysis(self, start_date: Optional[str] = None, end_date: Optional[str] = None) -> Dict:
        """
        è·å– insnap.ai é“¾æ¥æ¯æ—¥ç‚¹å‡»ä¸æµè§ˆè¶‹åŠ¿åˆ†ææ•°æ®ï¼ŒåŒ…å«PVï¼ˆç‚¹å‡»é‡ï¼‰å’ŒUVï¼ˆç‹¬ç«‹è®¿å®¢æ•°ï¼‰
        """
        if self.clicks_df is None or self.merged_df is None:
            return {}
        
        link_group_mapping = {
            'https://insnap.ai/videos': 'yujie_main_avatar',
            'https://insnap.ai/zh/download': 'wan_produce101'
        }
        result = {}
        for link_url, target_group in link_group_mapping.items():
            print(f"ğŸ” åˆ†æé“¾æ¥: {link_url} -> ç›®æ ‡åˆ†ç»„: {target_group}")
            # 1. è·å–é“¾æ¥ç‚¹å‡»æ•°æ®
            link_clicks = self.clicks_df.copy()
            if 'timestamp' in link_clicks.columns:
                link_clicks['date'] = pd.to_datetime(link_clicks['timestamp']).dt.date
            elif 'date' in link_clicks.columns:
                link_clicks['date'] = pd.to_datetime(link_clicks['date']).dt.date
            else:
                print(f"âŒ ç‚¹å‡»æ•°æ®ä¸­æœªæ‰¾åˆ°æ—¶é—´å­—æ®µ")
                continue
            http_link = link_url.replace('https://', 'http://')
            https_link = link_url.replace('http://', 'https://')
            link_clicks = link_clicks[
                (link_clicks['page_url'] == link_url) |
                (link_clicks['page_url'] == http_link) |
                (link_clicks['page_url'] == https_link)
            ].copy()
            if start_date:
                start_date_ts = pd.Timestamp(start_date)
                link_clicks = link_clicks[link_clicks['date'] >= start_date_ts.date()]
            if end_date:
                end_date_ts = pd.Timestamp(end_date)
                link_clicks = link_clicks[link_clicks['date'] <= end_date_ts.date()]
            
            # æ¯æ—¥ç‚¹å‡»é‡ï¼ˆPVï¼‰å’Œè®¿å®¢æ•°ï¼ˆUVï¼‰
            if not link_clicks.empty:
                # æ¯æ—¥ç‚¹å‡»é‡ï¼ˆPVï¼‰- session_idå»é‡
                daily_clicks = link_clicks.groupby('date')['session_id'].nunique().reset_index(name='daily_clicks')
                daily_clicks['date'] = pd.to_datetime(daily_clicks['date'])
                
                # æ¯æ—¥è®¿å®¢æ•°ï¼ˆUVï¼‰- visitor_idå»é‡
                daily_visitors = link_clicks.groupby('date')['visitor_id'].nunique().reset_index(name='daily_visitors')
                daily_visitors['date'] = pd.to_datetime(daily_visitors['date'])
                
                # åˆå¹¶PVå’ŒUVæ•°æ®
                daily_pv_uv = pd.merge(daily_clicks, daily_visitors, on='date', how='outer').fillna(0)
            else:
                daily_pv_uv = pd.DataFrame(columns=['date', 'daily_clicks', 'daily_visitors'])
            
            # 2. è·å–ç›®æ ‡åˆ†ç»„çš„æ¯æ—¥æµè§ˆé‡ï¼ˆview_diffï¼‰
            group_views = self.merged_df.copy()
            group_views['date'] = pd.to_datetime(group_views['date'])
            group_views = group_views[group_views['group'].str.contains(target_group, na=False, case=False)]
            if start_date:
                group_views = group_views[group_views['date'] >= start_date_ts]
            if end_date:
                group_views = group_views[group_views['date'] <= end_date_ts]
            # ç”¨ view_diff å­—æ®µï¼Œè‹¥æ— åˆ™å…¨ä¸º 0
            if 'view_diff' in group_views.columns:
                group_views['view_diff'] = pd.to_numeric(group_views['view_diff'], errors='coerce').fillna(0)
                daily_views = group_views.groupby(group_views['date'].dt.date)['view_diff'].sum().reset_index()
                daily_views.columns = ['date', 'daily_views']
                daily_views['date'] = pd.to_datetime(daily_views['date'])
            else:
                print(f"âš ï¸ merged_df æ—  view_diff å­—æ®µï¼Œå…¨éƒ¨è§†ä¸º 0")
                daily_views = pd.DataFrame(columns=['date', 'daily_views'])
            
            # 3. åˆå¹¶æ‰€æœ‰æ•°æ®
            merged_data = pd.merge(daily_pv_uv, daily_views, on='date', how='outer').fillna(0)
            merged_data['date'] = pd.to_datetime(merged_data['date']).dt.date
            
            # 4. è®¡ç®—æ¯æ—¥è½¬åŒ–ç‡
            merged_data['daily_pv_conversion_rate'] = merged_data.apply(
                lambda row: round(row['daily_clicks'] / row['daily_views'] * 100, 2) if row['daily_views'] > 0 else 0.0,
                axis=1
            )
            merged_data['daily_uv_conversion_rate'] = merged_data.apply(
                lambda row: round(row['daily_visitors'] / row['daily_views'] * 100, 2) if row['daily_views'] > 0 else 0.0,
                axis=1
            )
            
            # 5. æ—¥æœŸè¿ç»­åŒ–
            if not merged_data.empty:
                all_dates = pd.date_range(merged_data['date'].min(), merged_data['date'].max())
                merged_data = merged_data.set_index('date').reindex(all_dates, fill_value=0).reset_index()
                merged_data = merged_data.rename(columns={'index': 'date'})
                merged_data['date'] = pd.to_datetime(merged_data['date']).dt.date
            
            # 6. ä»Šæ—¥æ–°å¢æ•°æ®
            today_row = merged_data.iloc[-1] if not merged_data.empty else None
            today_data = {
                'date': str(today_row['date']) if today_row is not None else '',
                'pv': int(today_row['daily_clicks']) if today_row is not None else 0,
                'uv': int(today_row['daily_visitors']) if today_row is not None else 0,
                'views': int(today_row['daily_views']) if today_row is not None else 0,
                'pv_rate': round(today_row['daily_clicks'] / today_row['daily_views'] * 100, 2) if today_row is not None and today_row['daily_views'] > 0 else 0.0,
                'uv_rate': round(today_row['daily_visitors'] / today_row['daily_views'] * 100, 2) if today_row is not None and today_row['daily_views'] > 0 else 0.0
            }
            
            # 7. æ±‡æ€»ç»“æœ
            result[link_url] = {
                'target_group': target_group,
                'total_clicks': int(link_clicks['session_id'].nunique()) if not link_clicks.empty else 0,
                'total_visitors': int(link_clicks['visitor_id'].nunique()) if not link_clicks.empty else 0,
                'total_views': int(group_views['view_count'].sum()) if 'view_count' in group_views.columns else 0,
                'avg_pv_conversion_rate': merged_data['daily_pv_conversion_rate'].mean() if not merged_data.empty else 0.0,
                'avg_uv_conversion_rate': merged_data['daily_uv_conversion_rate'].mean() if not merged_data.empty else 0.0,
                'data': merged_data,
                'today': today_data
            }
            print(f"âœ… {link_url}: æ€»ç‚¹å‡»(PV) {result[link_url]['total_clicks']}, æ€»è®¿å®¢(UV) {result[link_url]['total_visitors']}, æ€»æµè§ˆ {result[link_url]['total_views']}")
            print(f"   å¹³å‡PVè½¬åŒ–ç‡ {result[link_url]['avg_pv_conversion_rate']:.2f}%, å¹³å‡UVè½¬åŒ–ç‡ {result[link_url]['avg_uv_conversion_rate']:.2f}%")
        return result

    def get_latest_day_increment_metrics(self, target_date: Optional[str] = None) -> Dict:
        """
        è·å–æŒ‡å®šæ—¥æœŸï¼ˆæˆ–æœ€æ–°æ—¥æœŸï¼‰çš„æ¯æ—¥å¢é‡æŒ‡æ ‡
        
        Args:
            target_date (Optional[str]): ç›®æ ‡æ—¥æœŸï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æœ€æ–°æ—¥æœŸ
        
        Returns:
            Dict: åŒ…å«å„é¡¹å¢é‡æŒ‡æ ‡çš„å­—å…¸
        """
        if self.merged_df is None:
            return {}
        
        try:
            # ç¡®å®šç›®æ ‡æ—¥æœŸ
            if target_date:
                target_date = pd.Timestamp(target_date)
            else:
                # ä½¿ç”¨æœ€æ–°æ—¥æœŸ
                target_date = self.merged_df['date'].max()
            
            # è·å–ç›®æ ‡æ—¥æœŸçš„æ•°æ®
            target_data = self.merged_df[self.merged_df['date'] == target_date].copy()
            
            if target_data.empty:
                return {}
            
            # è®¡ç®—å¢é‡æŒ‡æ ‡
            # ä½¿ç”¨ diff åˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰æˆ–è®¡ç®—å·®å€¼
            increment_metrics = {}
            
            # å¤„ç†è§‚çœ‹é‡å¢é‡
            if 'view_diff' in target_data.columns:
                increment_metrics['view_increment'] = int(target_data['view_diff'].sum())
            else:
                # å¦‚æœæ²¡æœ‰ view_diffï¼Œå°è¯•è®¡ç®—å·®å€¼
                increment_metrics['view_increment'] = 0
            
            # å¤„ç†å‘å¸–é‡å¢é‡
            if 'post_diff' in target_data.columns:
                increment_metrics['post_increment'] = int(target_data['post_diff'].sum())
            else:
                increment_metrics['post_increment'] = 0
            
            # å¤„ç†ç‚¹èµæ•°å¢é‡
            if 'like_diff' in target_data.columns:
                increment_metrics['like_increment'] = int(target_data['like_diff'].sum())
            else:
                increment_metrics['like_increment'] = 0
            
            # å¤„ç†è¯„è®ºæ•°å¢é‡
            if 'comment_diff' in target_data.columns:
                increment_metrics['comment_increment'] = int(target_data['comment_diff'].sum())
            else:
                increment_metrics['comment_increment'] = 0
            
            # å¤„ç†åˆ†äº«æ•°å¢é‡
            if 'share_diff' in target_data.columns:
                increment_metrics['share_increment'] = int(target_data['share_diff'].sum())
            else:
                increment_metrics['share_increment'] = 0
            
            # å¤„ç†ç‚¹å‡»æ•°å¢é‡ï¼ˆä»ç‚¹å‡»æ•°æ®è·å–ï¼‰
            if self.clicks_df is not None:
                clicks_target_data = self.clicks_df.copy()
                if 'timestamp' in clicks_target_data.columns:
                    clicks_target_data['date'] = pd.to_datetime(clicks_target_data['timestamp']).dt.date
                elif 'date' in clicks_target_data.columns:
                    clicks_target_data['date'] = pd.to_datetime(clicks_target_data['date']).dt.date
                
                clicks_target_data = clicks_target_data[clicks_target_data['date'] == target_date.date()]
                increment_metrics['click_increment'] = len(clicks_target_data)
            else:
                increment_metrics['click_increment'] = 0
            
            # æ·»åŠ ç›®æ ‡æ—¥æœŸä¿¡æ¯
            increment_metrics['target_date'] = target_date.strftime('%Y-%m-%d')
            
            return increment_metrics
            
        except Exception as e:
            print(f"âŒ è·å–æœ€æ–°ä¸€å¤©å¢é‡æŒ‡æ ‡å¤±è´¥: {str(e)}")
            return {}

    def get_last_day_clicks_summary(self, start_date: Optional[str] = None, end_date: Optional[str] = None) -> Dict:
        """
        è·å–æœ€åä¸€å¤©çš„ç‚¹å‡»ç»Ÿè®¡æ•°æ®ï¼ˆPVå’ŒUVï¼‰
        
        Args:
            start_date (Optional[str]): å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'
            end_date (Optional[str]): ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'
        
        Returns:
            Dict: åŒ…å«ä¸¤ä¸ªé“¾æ¥çš„æœ€åä¸€å¤©ç‚¹å‡»ç»Ÿè®¡æ•°æ®
        """
        if self.clicks_df is None:
            print("âš ï¸ clicks_df ä¸ºç©º")
            return {}
        
        try:
            print(f"ğŸ” å¼€å§‹å¤„ç†ç‚¹å‡»æ•°æ®ï¼ŒåŸå§‹æ•°æ®è¡Œæ•°: {len(self.clicks_df)}")
            
            # ç­›é€‰æ—¥æœŸèŒƒå›´
            clicks_df = self.clicks_df.copy()
            
            # å¤„ç†æ—¥æœŸå­—æ®µ
            if 'timestamp' in clicks_df.columns:
                print("ğŸ“… ä½¿ç”¨ timestamp å­—æ®µä½œä¸ºæ—¥æœŸ")
                clicks_df['date'] = pd.to_datetime(clicks_df['timestamp']).dt.date
            elif 'date' in clicks_df.columns:
                print("ğŸ“… ä½¿ç”¨ date å­—æ®µä½œä¸ºæ—¥æœŸ")
                clicks_df['date'] = pd.to_datetime(clicks_df['date']).dt.date
            else:
                print("âŒ æœªæ‰¾åˆ°æ—¥æœŸå­—æ®µ")
                return {}
            
            print(f"ğŸ“Š æ—¥æœŸå¤„ç†åæ•°æ®è¡Œæ•°: {len(clicks_df)}")
            print(f"ğŸ“… æ—¥æœŸèŒƒå›´: {clicks_df['date'].min()} åˆ° {clicks_df['date'].max()}")
            
            # åªä¿ç•™æ—¥æœŸèŒƒå›´å†…çš„æ•°æ®
            if start_date:
                start_date_dt = pd.to_datetime(start_date).date()
                clicks_df = clicks_df[clicks_df['date'] >= start_date_dt]
                print(f"ğŸ“… åº”ç”¨å¼€å§‹æ—¥æœŸè¿‡æ»¤ {start_date_dt}ï¼Œå‰©ä½™è¡Œæ•°: {len(clicks_df)}")
            if end_date:
                end_date_dt = pd.to_datetime(end_date).date()
                clicks_df = clicks_df[clicks_df['date'] <= end_date_dt]
                print(f"ğŸ“… åº”ç”¨ç»“æŸæ—¥æœŸè¿‡æ»¤ {end_date_dt}ï¼Œå‰©ä½™è¡Œæ•°: {len(clicks_df)}")
            
            if clicks_df.empty:
                print("âš ï¸ æ—¥æœŸè¿‡æ»¤åæ•°æ®ä¸ºç©º")
                return {}
            
            # å–æœ€åä¸€å¤©
            last_day = clicks_df['date'].max()
            last_day_df = clicks_df[clicks_df['date'] == last_day]
            print(f"ğŸ“… æœ€åä¸€å¤©: {last_day}ï¼Œæ•°æ®è¡Œæ•°: {len(last_day_df)}")
            
            # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
            required_cols = ['page_url', 'session_id', 'visitor_id']
            missing_cols = [col for col in required_cols if col not in last_day_df.columns]
            if missing_cols:
                print(f"âŒ ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
                print(f"ğŸ“‹ å¯ç”¨åˆ—: {list(last_day_df.columns)}")
                return {}
            
            # é“¾æ¥æ˜ å°„ï¼Œå¿½ç•¥http/https
            link_targets = [
                'insnap.ai/videos',
                'insnap.ai/zh/download'
            ]
            
            result = {}
            for link in link_targets:
                print(f"ğŸ”— å¤„ç†é“¾æ¥: {link}")
                
                # å…¼å®¹http/httpsï¼Œå»é™¤åè®®åå†æ¯”å¯¹
                def normalize_url(url):
                    if pd.isna(url):
                        return ""
                    url_str = str(url)
                    # å»é™¤åè®®å’ŒæŸ¥è¯¢å‚æ•°
                    url_str = url_str.replace('http://', '').replace('https://', '').split('?')[0]
                    return url_str
                
                # åº”ç”¨URLæ ‡å‡†åŒ–
                last_day_df['normalized_url'] = last_day_df['page_url'].apply(normalize_url)
                mask = last_day_df['normalized_url'] == link
                link_df = last_day_df[mask]
                
                print(f"   ğŸ“Š åŒ¹é…åˆ° {len(link_df)} æ¡è®°å½•")
                
                if not link_df.empty:
                    # ç»Ÿè®¡PVå’ŒUV
                    pv = link_df['session_id'].nunique()
                    uv = link_df['visitor_id'].nunique()
                    
                    print(f"   ğŸ“ˆ PV (session_idå»é‡): {pv}")
                    print(f"   ğŸ‘¥ UV (visitor_idå»é‡): {uv}")
                    
                    result[f'https://{link}'] = {
                        'date': str(last_day),
                        'pv': pv,
                        'uv': uv
                    }
                else:
                    print(f"   âš ï¸ æœªæ‰¾åˆ°åŒ¹é…è®°å½•")
                    result[f'https://{link}'] = {
                        'date': str(last_day),
                        'pv': 0,
                        'uv': 0
                    }
            
            print(f"âœ… å¤„ç†å®Œæˆï¼Œè¿”å›ç»“æœ: {result}")
            return result
            
        except Exception as e:
            print(f'âŒ get_last_day_clicks_summary error: {e}')
            import traceback
            traceback.print_exc()
            return {}

    def get_top_performing_accounts(self, start_date: Optional[str] = None, end_date: Optional[str] = None, top_n: int = 5) -> pd.DataFrame:
        """
        è·å–è¡¨ç°æœ€å¥½çš„è´¦å·Top Nï¼ˆæŒ‰æœ€åä¸€å¤©æ–°å¢æµè§ˆé‡æ’åºï¼‰
        
        Args:
            start_date (Optional[str]): å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'
            end_date (Optional[str]): ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'
            top_n (int): è¿”å›å‰Nä¸ªè´¦å·ï¼Œé»˜è®¤5ä¸ª
        
        Returns:
            pd.DataFrame: åŒ…å«è´¦å·ä¿¡æ¯çš„DataFrame
        """
        if self.merged_df is None or self.accounts_df is None:
            return pd.DataFrame()
        
        try:
            # ç­›é€‰æ—¥æœŸèŒƒå›´
            df = self.merged_df.copy()
            if start_date:
                start_date_ts = pd.Timestamp(start_date)
                df = df[df['date'] >= start_date_ts]
            if end_date:
                end_date_ts = pd.Timestamp(end_date)
                df = df[df['date'] <= end_date_ts]
            
            # åªå–æœ€åä¸€å¤©çš„æ•°æ®
            last_date = df['date'].max()
            last_day_data = df[df['date'] == last_date].copy()
            
            print(f"ğŸ“… ä½¿ç”¨æœ€åä¸€å¤©æ•°æ®: {last_date.strftime('%Y-%m-%d')}")
            
            if last_day_data.empty:
                print("âš ï¸ æœ€åä¸€å¤©æ²¡æœ‰æ•°æ®")
                return pd.DataFrame()
            
            # æŒ‰user_idèšåˆæœ€åä¸€å¤©çš„view_diff
            if 'view_diff' in last_day_data.columns:
                # ç¡®ä¿view_diffä¸ºæ•°å€¼ç±»å‹
                last_day_data['view_diff'] = pd.to_numeric(last_day_data['view_diff'], errors='coerce').fillna(0)
                
                # æŒ‰user_idèšåˆæœ€åä¸€å¤©çš„æ•°æ®
                account_performance = last_day_data.groupby('user_id').agg({
                    'view_diff': 'sum'  # åªç»Ÿè®¡æœ€åä¸€å¤©çš„æ–°å¢æµè§ˆé‡
                }).reset_index()
                
                # æŒ‰view_diffé™åºæ’åºï¼Œå–å‰Nä¸ª
                account_performance = account_performance.sort_values('view_diff', ascending=False).head(top_n)
                
                # å‡†å¤‡accounts_dfç”¨äºåˆå¹¶
                accounts_info = self.accounts_df.copy()
                accounts_info['user_id'] = accounts_info['Tiktok ID'].astype(str)
                
                # åˆå¹¶è´¦å·ä¿¡æ¯
                result = account_performance.merge(
                    accounts_info[['user_id', 'Tiktok Username', 'Total Followers', 'Total Like']], 
                    on='user_id', 
                    how='left'
                )
                
                # å¤„ç†ç¼ºå¤±å€¼
                result['Tiktok Username'] = result['Tiktok Username'].fillna('æœªçŸ¥')
                result['Total Followers'] = pd.to_numeric(result['Total Followers'], errors='coerce').fillna(0)
                result['Total Like'] = pd.to_numeric(result['Total Like'], errors='coerce').fillna(0)
                
                # æ„é€ ä¸»é¡µé“¾æ¥
                result['profile_url'] = result['Tiktok Username'].apply(
                    lambda x: f"https://www.tiktok.com/@{x}" if x != 'æœªçŸ¥' else ''
                )
                
                # é‡å‘½ååˆ—
                result = result.rename(columns={
                    'Tiktok Username': 'username',
                    'Total Followers': 'follower_count',
                    'Total Like': 'like_count',
                    'view_diff': 'last_day_view_increment'
                })
                
                # é€‰æ‹©å¹¶æ’åºåˆ—
                result = result[[
                    'profile_url', 'user_id', 'username', 'last_day_view_increment', 
                    'follower_count', 'like_count'
                ]]
                
                print(f"âœ… æˆåŠŸè·å–Top {len(result)} è´¦å·ï¼ŒåŸºäº {last_date.strftime('%Y-%m-%d')} çš„æ–°å¢æµè§ˆé‡")
                return result
            else:
                print("âš ï¸ æ•°æ®ä¸­æœªæ‰¾åˆ°view_diffå­—æ®µ")
                return pd.DataFrame()
                
        except Exception as e:
            print(f"âŒ è·å–è¡¨ç°æœ€å¥½è´¦å·å¤±è´¥: {str(e)}")
            return pd.DataFrame() 

    def get_selected_groups_latest_increments(self, selected_groups: List[str], target_date: Optional[str] = None) -> Dict:
        """
        è·å–å½“å‰é€‰ä¸­åˆ†ç»„çš„æœ€æ–°ä¸€å¤©å¢é‡æŒ‡æ ‡
        
        Args:
            selected_groups (List[str]): é€‰ä¸­çš„åˆ†ç»„å…³é”®è¯åˆ—è¡¨
            target_date (Optional[str]): ç›®æ ‡æ—¥æœŸï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'ï¼Œé»˜è®¤ä¸ºæœ€æ–°æ—¥æœŸ
        
        Returns:
            Dict: åŒ…å«æ¯ä¸ªåˆ†ç»„çš„å¢é‡æŒ‡æ ‡æ•°æ®
        """
        if self.merged_df is None or not selected_groups:
            return {}
        
        try:
            # è·å–ç›®æ ‡æ—¥æœŸ
            if target_date:
                target_date = pd.Timestamp(target_date)
            else:
                target_date = self.merged_df['date'].max()
            
            print(f"ğŸ“… è·å–åˆ†ç»„å¢é‡æŒ‡æ ‡ï¼Œç›®æ ‡æ—¥æœŸ: {target_date.strftime('%Y-%m-%d')}")
            
            # ç­›é€‰ç›®æ ‡æ—¥æœŸçš„æ•°æ®
            target_data = self.merged_df[self.merged_df['date'] == target_date].copy()
            
            if target_data.empty:
                print(f"âš ï¸ ç›®æ ‡æ—¥æœŸ {target_date.strftime('%Y-%m-%d')} æ— æ•°æ®")
                return {}
            
            result = {}
            
            for group_keyword in selected_groups:
                print(f"ğŸ” å¤„ç†åˆ†ç»„: {group_keyword}")
                
                # ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…ç­›é€‰åˆ†ç»„æ•°æ®
                group_data = target_data[target_data['group'].str.contains(group_keyword, na=False, case=False)]
                
                if group_data.empty:
                    print(f"   âš ï¸ åˆ†ç»„ '{group_keyword}' æ— åŒ¹é…æ•°æ®")
                    result[group_keyword] = {
                        'date': target_date.strftime('%Y-%m-%d'),
                        'posts': 0,
                        'views': 0,
                        'likes': 0,
                        'comments': 0,
                        'shares': 0
                    }
                    continue
                
                print(f"   ğŸ“Š æ‰¾åˆ° {len(group_data)} æ¡è®°å½•")
                
                # è®¡ç®—å„é¡¹å¢é‡æŒ‡æ ‡
                group_metrics = {
                    'date': target_date.strftime('%Y-%m-%d'),
                    'posts': 0,
                    'views': 0,
                    'likes': 0,
                    'comments': 0,
                    'shares': 0
                }
                
                # å¤„ç†å‘å¸–é‡å¢é‡
                if 'post_diff' in group_data.columns:
                    group_metrics['posts'] = int(group_data['post_diff'].sum())
                
                # å¤„ç†æµè§ˆé‡å¢é‡
                if 'view_diff' in group_data.columns:
                    group_metrics['views'] = int(group_data['view_diff'].sum())
                
                # å¤„ç†ç‚¹èµæ•°å¢é‡
                if 'like_diff' in group_data.columns:
                    group_metrics['likes'] = int(group_data['like_diff'].sum())
                
                # å¤„ç†è¯„è®ºæ•°å¢é‡
                if 'comment_diff' in group_data.columns:
                    group_metrics['comments'] = int(group_data['comment_diff'].sum())
                
                # å¤„ç†åˆ†äº«æ•°å¢é‡
                if 'share_diff' in group_data.columns:
                    group_metrics['shares'] = int(group_data['share_diff'].sum())
                
                result[group_keyword] = group_metrics
                
                print(f"   ğŸ“ˆ å¢é‡æŒ‡æ ‡: å‘å¸–{group_metrics['posts']}, æµè§ˆ{group_metrics['views']}, ç‚¹èµ{group_metrics['likes']}, è¯„è®º{group_metrics['comments']}, åˆ†äº«{group_metrics['shares']}")
            
            return result
            
        except Exception as e:
            print(f"âŒ è·å–åˆ†ç»„å¢é‡æŒ‡æ ‡å¤±è´¥: {str(e)}")
            return {} 