import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, timedelta, date
import warnings
import sys
import os
import requests

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ•°æ®åˆå¹¶è°ƒè¯•",
    page_icon="ğŸ”",
    layout="wide"
)

st.title("ğŸ” æ•°æ®åˆå¹¶é—®é¢˜è°ƒè¯•")

# æ·»åŠ  scripts ç›®å½•åˆ° Python è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
scripts_dir = os.path.join(parent_dir, 'scripts')

if os.path.exists(scripts_dir):
    sys.path.insert(0, scripts_dir)

try:
    from enhanced_data_processor import EnhancedTikTokDataProcessor
    st.success("âœ… æ¨¡å—å¯¼å…¥æˆåŠŸ")
except Exception as e:
    st.error(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    st.stop()

@st.cache_data
def load_accounts_data():
    try:
        local_path = "data/postingManager_data/accounts_detail.xlsx"
        if os.path.exists(local_path):
            df = pd.read_excel(local_path)
            st.write("[DEBUG] æœ¬åœ° accounts_detail.xlsx åŠ è½½æˆåŠŸï¼Œshape:", df.shape)
            return df
        
        # å°è¯•ä»äº‘ç«¯åŠ è½½
        try:
            try:
                url = st.secrets["ACCOUNTS_URL"]
            except Exception as secrets_error:
                st.warning(f"æœ¬åœ°è¿è¡Œæ—¶æ— æ³•è·å–äº‘ç«¯æ•°æ®URL: {secrets_error}")
                st.info("ğŸ’¡ æç¤ºï¼šè¿™æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸ºæœ¬åœ°è¿è¡Œæ—¶æ²¡æœ‰è®¾ç½®secretsã€‚åœ¨Streamlit Cloudä¸Šéƒ¨ç½²æ—¶ä¼šè‡ªåŠ¨ä»äº‘ç«¯åŠ è½½æ•°æ®ã€‚")
                return None
            response = requests.get(url)
            response.raise_for_status()
            tmp_path = "/tmp/accounts_detail.xlsx"
            with open(tmp_path, "wb") as f:
                f.write(response.content)
            st.write(f"[DEBUG] ä»äº‘ç«¯ä¸‹è½½ accounts_detail.xlsx")
            df = pd.read_excel(tmp_path)
            st.write("[DEBUG] äº‘ç«¯ accounts_detail.xlsx åŠ è½½æˆåŠŸï¼Œshape:", df.shape)
            return df
        except Exception as e:
            st.error(f"äº‘ç«¯æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return None
    except Exception as e:
        st.error(f"è´¦å·æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None

@st.cache_data
def load_redash_data():
    try:
        local_path = "data/redash_data/redash_data_2025-07-14.csv"
        if os.path.exists(local_path):
            df = pd.read_csv(local_path)
            st.write("[DEBUG] æœ¬åœ° redash_data_2025-07-14.csv åŠ è½½æˆåŠŸï¼Œshape:", df.shape)
            return df
        
        # å°è¯•ä»äº‘ç«¯åŠ è½½
        try:
            try:
                url = st.secrets["REDASH_URL"]
            except Exception as secrets_error:
                st.warning(f"æœ¬åœ°è¿è¡Œæ—¶æ— æ³•è·å–äº‘ç«¯æ•°æ®URL: {secrets_error}")
                st.info("ğŸ’¡ æç¤ºï¼šè¿™æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸ºæœ¬åœ°è¿è¡Œæ—¶æ²¡æœ‰è®¾ç½®secretsã€‚åœ¨Streamlit Cloudä¸Šéƒ¨ç½²æ—¶ä¼šè‡ªåŠ¨ä»äº‘ç«¯åŠ è½½æ•°æ®ã€‚")
                return None
            response = requests.get(url)
            response.raise_for_status()
            tmp_path = "/tmp/redash_data.csv"
            with open(tmp_path, "wb") as f:
                f.write(response.content)
            st.write(f"[DEBUG] ä»äº‘ç«¯ä¸‹è½½ redash_data.csv")
            df = pd.read_csv(tmp_path)
            st.write("[DEBUG] äº‘ç«¯ redash_data.csv åŠ è½½æˆåŠŸï¼Œshape:", df.shape)
            return df
        except Exception as e:
            st.error(f"äº‘ç«¯æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return None
    except Exception as e:
        st.error(f"Redashæ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None

@st.cache_data
def load_clicks_data():
    try:
        local_path = "data/clicks/your_clicks_file.csv"
        if os.path.exists(local_path):
            df = pd.read_csv(local_path)
            st.write("[DEBUG] æœ¬åœ° clicks åŠ è½½æˆåŠŸï¼Œshape:", df.shape)
            return df
        
        # å°è¯•ä»äº‘ç«¯åŠ è½½
        try:
            try:
                url = st.secrets["CLICKS_URL"]
            except Exception as secrets_error:
                st.warning(f"æœ¬åœ°è¿è¡Œæ—¶æ— æ³•è·å–äº‘ç«¯æ•°æ®URL: {secrets_error}")
                st.info("ğŸ’¡ æç¤ºï¼šè¿™æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸ºæœ¬åœ°è¿è¡Œæ—¶æ²¡æœ‰è®¾ç½®secretsã€‚åœ¨Streamlit Cloudä¸Šéƒ¨ç½²æ—¶ä¼šè‡ªåŠ¨ä»äº‘ç«¯åŠ è½½æ•°æ®ã€‚")
                return None
            response = requests.get(url)
            response.raise_for_status()
            tmp_path = "/tmp/clicks.csv"
            with open(tmp_path, "wb") as f:
                f.write(response.content)
            st.write(f"[DEBUG] ä»äº‘ç«¯ä¸‹è½½ clicks.csv")
            df = pd.read_csv(tmp_path)
            st.write("[DEBUG] äº‘ç«¯ clicks.csv åŠ è½½æˆåŠŸï¼Œshape:", df.shape)
            return df
        except Exception as e:
            st.error(f"äº‘ç«¯æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return None
    except Exception as e:
        st.error(f"ç‚¹å‡»æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None

# æ•°æ®åŠ è½½æµ‹è¯•
st.header("ğŸ“Š æ•°æ®åŠ è½½æµ‹è¯•")

accounts_df = load_accounts_data()
redash_df = load_redash_data()
clicks_df = load_clicks_data()

# æ˜¾ç¤ºæ•°æ®è¯¦æƒ…
if accounts_df is not None:
    st.subheader("ğŸ“‹ Accounts æ•°æ®è¯¦æƒ…")
    st.write(f"Shape: {accounts_df.shape}")
    st.write(f"Columns: {accounts_df.columns.tolist()}")
    st.write("å‰5è¡Œæ•°æ®:")
    st.dataframe(accounts_df.head())
    
    # æ£€æŸ¥å…³é”®åˆ—
    if 'Tiktok ID' in accounts_df.columns:
        st.success("âœ… æ‰¾åˆ° 'Tiktok ID' åˆ—")
        st.write(f"Tiktok ID å”¯ä¸€å€¼æ•°é‡: {accounts_df['Tiktok ID'].nunique()}")
    else:
        st.error("âŒ ç¼ºå°‘ 'Tiktok ID' åˆ—")
    
    if 'Groups' in accounts_df.columns:
        st.success("âœ… æ‰¾åˆ° 'Groups' åˆ—")
        st.write(f"Groups å”¯ä¸€å€¼æ•°é‡: {accounts_df['Groups'].nunique()}")
    else:
        st.error("âŒ ç¼ºå°‘ 'Groups' åˆ—")

if redash_df is not None:
    st.subheader("ğŸ“‹ Redash æ•°æ®è¯¦æƒ…")
    st.write(f"Shape: {redash_df.shape}")
    st.write(f"Columns: {redash_df.columns.tolist()}")
    st.write("å‰5è¡Œæ•°æ®:")
    st.dataframe(redash_df.head())
    
    # æ£€æŸ¥å…³é”®åˆ—
    if 'user_id' in redash_df.columns:
        st.success("âœ… æ‰¾åˆ° 'user_id' åˆ—")
        st.write(f"user_id å”¯ä¸€å€¼æ•°é‡: {redash_df['user_id'].nunique()}")
    else:
        st.error("âŒ ç¼ºå°‘ 'user_id' åˆ—")

if clicks_df is not None:
    st.subheader("ğŸ“‹ Clicks æ•°æ®è¯¦æƒ…")
    st.write(f"Shape: {clicks_df.shape}")
    st.write(f"Columns: {clicks_df.columns.tolist()}")
    st.write("å‰5è¡Œæ•°æ®:")
    st.dataframe(clicks_df.head())

# æ•°æ®åˆå¹¶æµ‹è¯•
st.header("ğŸ”— æ•°æ®åˆå¹¶æµ‹è¯•")

# å¦‚æœæ²¡æœ‰çœŸå®æ•°æ®ï¼Œåˆ›å»ºæ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæµ‹è¯•
if accounts_df is None or redash_df is None:
    st.warning("âš ï¸ ç¼ºå°‘çœŸå®æ•°æ®æ–‡ä»¶ï¼Œåˆ›å»ºæ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæµ‹è¯•")
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    import numpy as np
    from datetime import datetime, timedelta
    
    # æ¨¡æ‹Ÿ accounts æ•°æ®
    st.subheader("ğŸ“‹ åˆ›å»ºæ¨¡æ‹Ÿ Accounts æ•°æ®")
    mock_accounts_data = {
        'Tiktok ID': [f'user_{i:03d}' for i in range(1, 21)],
        'Groups': ['yujie_main_avatar'] * 10 + ['wan_produce101'] * 10,
        'username': [f'user_{i:03d}' for i in range(1, 21)],
        'follower_count': np.random.randint(1000, 100000, 20),
        'like_count': np.random.randint(100, 10000, 20)
    }
    accounts_df = pd.DataFrame(mock_accounts_data)
    st.write("æ¨¡æ‹Ÿ accounts æ•°æ®:")
    st.dataframe(accounts_df.head())
    
    # æ¨¡æ‹Ÿ redash æ•°æ®
    st.subheader("ğŸ“‹ åˆ›å»ºæ¨¡æ‹Ÿ Redash æ•°æ®")
    dates = pd.date_range(start='2025-01-01', end='2025-01-31', freq='D')
    mock_redash_data = []
    
    for date in dates:
        for user_id in mock_accounts_data['Tiktok ID']:
            mock_redash_data.append({
                'date': date,
                'user_id': user_id,
                'view_count': np.random.randint(1000, 50000),
                'like_count': np.random.randint(100, 5000),
                'comment_count': np.random.randint(10, 500),
                'share_count': np.random.randint(5, 200),
                'post_count': np.random.randint(1, 10),
                'view_diff': np.random.randint(-1000, 5000),
                'like_diff': np.random.randint(-100, 500),
                'comment_diff': np.random.randint(-10, 50),
                'share_diff': np.random.randint(-5, 20),
                'post_diff': np.random.randint(-1, 3)
            })
    
    redash_df = pd.DataFrame(mock_redash_data)
    st.write("æ¨¡æ‹Ÿ redash æ•°æ®:")
    st.dataframe(redash_df.head())
    
    st.success("âœ… æ¨¡æ‹Ÿæ•°æ®åˆ›å»ºæˆåŠŸï¼Œç»§ç»­æµ‹è¯•åˆå¹¶åŠŸèƒ½")

# åˆ›å»º group_mapping
st.subheader("ğŸ“‹ Group Mapping åˆ›å»º")
try:
    group_mapping = accounts_df[['Tiktok ID', 'Groups']].drop_duplicates()
    group_mapping = group_mapping.rename(columns={'Tiktok ID': 'user_id', 'Groups': 'group'})
    group_mapping['user_id'] = group_mapping['user_id'].astype(str)
    group_mapping['group'] = group_mapping['group'].fillna('Unknown')
    
    st.write(f"Group mapping shape: {group_mapping.shape}")
    st.write("Group mapping å‰5è¡Œ:")
    st.dataframe(group_mapping.head())
    
    st.success("âœ… Group mapping åˆ›å»ºæˆåŠŸ")
except Exception as e:
    st.error(f"âŒ Group mapping åˆ›å»ºå¤±è´¥: {e}")
    st.stop()

# æµ‹è¯•åˆå¹¶
st.subheader("ğŸ”— æ•°æ®åˆå¹¶æµ‹è¯•")
try:
    # ç¡®ä¿ redash_df çš„ user_id ä¸ºå­—ç¬¦ä¸²
    redash_df['user_id'] = redash_df['user_id'].astype(str)
    
    # æ‰§è¡Œåˆå¹¶
    merged_df = redash_df.merge(group_mapping, on='user_id', how='left')
    
    st.write(f"åˆå¹¶å‰ redash_df shape: {redash_df.shape}")
    st.write(f"åˆå¹¶å‰ group_mapping shape: {group_mapping.shape}")
    st.write(f"åˆå¹¶å merged_df shape: {merged_df.shape}")
    
    if merged_df.empty:
        st.error("âŒ åˆå¹¶åæ•°æ®ä¸ºç©º")
    else:
        st.success("âœ… æ•°æ®åˆå¹¶æˆåŠŸ")
        st.write("åˆå¹¶åæ•°æ®å‰5è¡Œ:")
        st.dataframe(merged_df.head())
        
        # æ£€æŸ¥åˆ†ç»„ä¿¡æ¯
        if 'group' in merged_df.columns:
            st.write(f"åˆ†ç»„æ•°é‡: {merged_df['group'].nunique()}")
            st.write("åˆ†ç»„ç»Ÿè®¡:")
            st.write(merged_df['group'].value_counts())
        
except Exception as e:
    st.error(f"âŒ æ•°æ®åˆå¹¶å¤±è´¥: {e}")
    st.write("é”™è¯¯è¯¦æƒ…:")
    st.exception(e)

# æµ‹è¯•æ•°æ®å¤„ç†å™¨
st.header("ğŸ”§ æ•°æ®å¤„ç†å™¨æµ‹è¯•")
try:
    processor = EnhancedTikTokDataProcessor(
        accounts_df=accounts_df,
        redash_df=redash_df,
        clicks_df=clicks_df
    )
    st.success("âœ… æ•°æ®å¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
    
    # æµ‹è¯•åˆå¹¶
    merge_result = processor.merge_data()
    st.write(f"åˆå¹¶ç»“æœ: {merge_result}")
    
    if processor.merged_df is not None:
        st.write(f"å¤„ç†å™¨åˆå¹¶å shape: {processor.merged_df.shape}")
        st.success("âœ… å¤„ç†å™¨åˆå¹¶æˆåŠŸ")
    else:
        st.error("âŒ å¤„ç†å™¨åˆå¹¶å¤±è´¥")
        
except Exception as e:
    st.error(f"âŒ æ•°æ®å¤„ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
    st.exception(e) 