import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, timedelta, date
import warnings
import sys
import os
import altair as alt
import requests
import pickle

# ç¯å¢ƒå’Œæ•°æ®å®Œæ•´æ€§è°ƒè¯•è¾“å‡º
st.write('Python version:', sys.version)
try:
    import streamlit
    st.write('Streamlit version:', streamlit.__version__)
except Exception as e:
    st.write('Streamlit import error:', e)

st.write('Current working dir:', os.getcwd())
# è‡ªåŠ¨åˆ›å»ºæ‰€éœ€çš„ç©ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
for d in [
    "data",
    "data/redash_data",
    "data/clicks",
    "data/postingManager_data"
]:
    os.makedirs(d, exist_ok=True)

# æ£€æŸ¥ç›®å½•å†…å®¹ï¼Œä½¿ç”¨æ›´å¥å£®çš„è·¯å¾„å¤„ç†
st.write(f'[DEBUG] å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}')
st.write(f'[DEBUG] å½“å‰ç›®å½•å†…å®¹: {os.listdir(".")}')

try:
    if os.path.exists('data'):
        st.write('Files in data/:', os.listdir('data'))
        # æ£€æŸ¥å­ç›®å½•
        for subdir in ['redash_data', 'clicks', 'postingManager_data']:
            subdir_path = os.path.join('data', subdir)
            if os.path.exists(subdir_path):
                st.write(f'Files in data/{subdir}/:', os.listdir(subdir_path))
            else:
                st.write(f'data/{subdir}/ ç›®å½•ä¸å­˜åœ¨')
    else:
        st.write('data/ ç›®å½•ä¸å­˜åœ¨')
except Exception as e:
    st.write('ç›®å½•è¯»å–å¤±è´¥:', e)

# æ·»åŠ  scripts ç›®å½•åˆ° Python è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
scripts_dir = os.path.join(parent_dir, 'scripts')

if os.path.exists(scripts_dir):
    sys.path.insert(0, scripts_dir)

from enhanced_data_processor import EnhancedTikTokDataProcessor
from enhanced_visualization import EnhancedVisualization

warnings.filterwarnings('ignore')

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="TikTok å¢å¼ºåˆ†æçœ‹æ¿",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #1f77b4;
    }
    .chart-container {
        background-color: white;
        padding: 1rem;
        border-radius: 0.5rem;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        margin-bottom: 1rem;
    }
    .stAlert {
        background-color: #e8f4fd;
        border-left: 4px solid #1f77b4;
    }
</style>
""", unsafe_allow_html=True)

import requests
import os
import pandas as pd

@st.cache_data
def load_accounts_data():
    try:
        # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
        possible_paths = [
            "data/postingManager_data/accounts_detail.xlsx",
            "../data/postingManager_data/accounts_detail.xlsx",
            "./data/postingManager_data/accounts_detail.xlsx",
            os.path.join(os.getcwd(), "data", "postingManager_data", "accounts_detail.xlsx")
        ]
        
        for local_path in possible_paths:
            if os.path.exists(local_path):
                df = pd.read_excel(local_path)
                st.write(f"[DEBUG] æœ¬åœ° accounts_detail.xlsx åŠ è½½æˆåŠŸï¼Œè·¯å¾„: {local_path}, shape: {df.shape}")
                return df
        
        st.write(f"[DEBUG] å°è¯•çš„è·¯å¾„: {possible_paths}")
        st.write(f"[DEBUG] å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
        st.write(f"[DEBUG] å½“å‰ç›®å½•å†…å®¹: {os.listdir('.')}")
        if os.path.exists('data'):
            st.write(f"[DEBUG] dataç›®å½•å†…å®¹: {os.listdir('data')}")
        else:
            st.write("[DEBUG] dataç›®å½•ä¸å­˜åœ¨")
        
        # å¦‚æœæœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•ä»äº‘ç«¯åŠ è½½
        
        # å°è¯•ä»äº‘ç«¯åŠ è½½
        try:
            try:
                url = st.secrets["ACCOUNTS_URL"]
            except Exception as secrets_error:
                st.error(f"æ— æ³•è·å–äº‘ç«¯æ•°æ®URL: {secrets_error}")
                return None
            response = requests.get(url)
            response.raise_for_status()  # æ£€æŸ¥HTTPé”™è¯¯
            tmp_path = "/tmp/accounts_detail.xlsx"
            with open(tmp_path, "wb") as f:
                f.write(response.content)
            st.write(f"[DEBUG] ä» {url} ä¸‹è½½ accounts_detail.xlsx åˆ° {tmp_path}")
            df = pd.read_excel(tmp_path)
            st.write("[DEBUG] äº‘ç«¯ accounts_detail.xlsx åŠ è½½æˆåŠŸï¼Œshape:", df.shape)
            return df
        except Exception as e:
            st.error(f"äº‘ç«¯æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return None
    except Exception as e:
        st.error(f"è´¦å·æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None

def load_redash_data():
    try:
        # åŠ¨æ€æŸ¥æ‰¾æœ€æ–°çš„ redash æ•°æ®æ–‡ä»¶
        def find_latest_redash_file():
            possible_dirs = [
                "data/redash_data",
                "../data/redash_data", 
                "./data/redash_data",
                os.path.join(os.getcwd(), "data", "redash_data")
            ]
            
            for data_dir in possible_dirs:
                if os.path.exists(data_dir):
                    redash_files = [f for f in os.listdir(data_dir) 
                                  if f.startswith('redash_data_') and f.endswith('.csv')]
                    if redash_files:
                        # åŸºäºæ–‡ä»¶åä¸­çš„æ—¥æœŸé€‰æ‹©æœ€æ–°æ–‡ä»¶
                        def extract_date_from_filename(filename):
                            try:
                                date_str = filename.replace('redash_data_', '').replace('.csv', '')
                                return pd.to_datetime(date_str)
                            except:
                                return pd.to_datetime('1900-01-01')
                        
                        latest_file = max(redash_files, key=extract_date_from_filename)
                        return os.path.join(data_dir, latest_file)
            return None
        
        latest_file_path = find_latest_redash_file()
        if latest_file_path:
            df = pd.read_csv(latest_file_path)
            st.write(f"[DEBUG] æœ¬åœ° redash æ•°æ®åŠ è½½æˆåŠŸï¼Œæ–‡ä»¶: {os.path.basename(latest_file_path)}, shape: {df.shape}")
            return df
        
        st.write(f"[DEBUG] æœªæ‰¾åˆ°æœ¬åœ° redash æ•°æ®æ–‡ä»¶")
        st.write(f"[DEBUG] å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
        if os.path.exists('data/redash_data'):
            st.write(f"[DEBUG] data/redash_dataç›®å½•å†…å®¹: {os.listdir('data/redash_data')}")
        else:
            st.write("[DEBUG] data/redash_dataç›®å½•ä¸å­˜åœ¨")
        
        # å¦‚æœæœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•ä»äº‘ç«¯åŠ è½½
        
        # å°è¯•ä»äº‘ç«¯åŠ è½½
        try:
            try:
                url = st.secrets["REDASH_URL"]
            except Exception as secrets_error:
                st.error(f"æ— æ³•è·å–äº‘ç«¯æ•°æ®URL: {secrets_error}")
                return None
            response = requests.get(url)
            response.raise_for_status()  # æ£€æŸ¥HTTPé”™è¯¯
            tmp_path = "/tmp/redash_data.csv"
            with open(tmp_path, "wb") as f:
                f.write(response.content)
            st.write(f"[DEBUG] ä» {url} ä¸‹è½½ redash_data.csv åˆ° {tmp_path}")
            df = pd.read_csv(tmp_path)
            st.write("[DEBUG] äº‘ç«¯ redash_data.csv åŠ è½½æˆåŠŸï¼Œshape:", df.shape)
            return df
        except Exception as e:
            st.error(f"äº‘ç«¯æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return None
    except Exception as e:
        st.error(f"Redashæ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None

def load_clicks_data():
    try:
        # åŠ¨æ€æŸ¥æ‰¾æœ€æ–°çš„ clicks æ•°æ®æ–‡ä»¶
        def find_latest_clicks_file():
            possible_dirs = [
                "data/clicks",
                "../data/clicks", 
                "./data/clicks",
                os.path.join(os.getcwd(), "data", "clicks")
            ]
            
            for data_dir in possible_dirs:
                if os.path.exists(data_dir):
                    clicks_files = [f for f in os.listdir(data_dir) 
                                  if f.endswith('.csv')]
                    if clicks_files:
                        # åŸºäºæ–‡ä»¶åä¸­çš„æ—¥æœŸé€‰æ‹©æœ€æ–°æ–‡ä»¶
                        def extract_date_from_clicks_filename(filename):
                            try:
                                date_str = filename[:8]  # å–å‰8ä½ä½œä¸ºæ—¥æœŸ
                                return pd.to_datetime(date_str, format='%Y%m%d')
                            except:
                                return pd.to_datetime('1900-01-01')
                        
                        latest_file = max(clicks_files, key=extract_date_from_clicks_filename)
                        return os.path.join(data_dir, latest_file)
            return None
        
        latest_file_path = find_latest_clicks_file()
        if latest_file_path:
            df = pd.read_csv(latest_file_path)
            st.write(f"[DEBUG] æœ¬åœ° clicks æ•°æ®åŠ è½½æˆåŠŸï¼Œæ–‡ä»¶: {os.path.basename(latest_file_path)}, shape: {df.shape}")
            return df
        
        st.write(f"[DEBUG] æœªæ‰¾åˆ°æœ¬åœ° clicks æ•°æ®æ–‡ä»¶")
        st.write(f"[DEBUG] å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
        if os.path.exists('data/clicks'):
            st.write(f"[DEBUG] data/clicksç›®å½•å†…å®¹: {os.listdir('data/clicks')}")
        else:
            st.write("[DEBUG] data/clicksç›®å½•ä¸å­˜åœ¨")
        
        # å¦‚æœæœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•ä»äº‘ç«¯åŠ è½½
        
        # å°è¯•ä»äº‘ç«¯åŠ è½½
        try:
            try:
                url = st.secrets["CLICKS_URL"]
            except Exception as secrets_error:
                st.error(f"æ— æ³•è·å–äº‘ç«¯æ•°æ®URL: {secrets_error}")
                return None
            response = requests.get(url)
            response.raise_for_status()  # æ£€æŸ¥HTTPé”™è¯¯
            tmp_path = "/tmp/clicks.csv"
            with open(tmp_path, "wb") as f:
                f.write(response.content)
            st.write(f"[DEBUG] ä» {url} ä¸‹è½½ clicks.csv åˆ° {tmp_path}")
            df = pd.read_csv(tmp_path)
            st.write("[DEBUG] äº‘ç«¯ clicks.csv åŠ è½½æˆåŠŸï¼Œshape:", df.shape)
            return df
        except Exception as e:
            st.error(f"äº‘ç«¯æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return None
    except Exception as e:
        st.error(f"ç‚¹å‡»æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None

# æ•°æ®åŠ è½½å’Œé”™è¯¯å¤„ç†
try:
    accounts_df = load_accounts_data()
    st.write(f"[DEBUG] accounts_df shape: {accounts_df.shape if accounts_df is not None else 'None'}")
    if accounts_df is not None:
        st.write(f"[DEBUG] accounts_df columns: {accounts_df.columns.tolist()}")
except Exception as e:
    st.error(f"âŒ è´¦å·æ•°æ®åŠ è½½å¤±è´¥: {e}")
    accounts_df = None

try:
    redash_df = load_redash_data()
    st.write(f"[DEBUG] redash_df shape: {redash_df.shape if redash_df is not None else 'None'}")
    if redash_df is not None:
        st.write(f"[DEBUG] redash_df columns: {redash_df.columns.tolist()}")
except Exception as e:
    st.error(f"âŒ Redashæ•°æ®åŠ è½½å¤±è´¥: {e}")
    redash_df = None

try:
    clicks_df = load_clicks_data()
    st.write(f"[DEBUG] clicks_df shape: {clicks_df.shape if clicks_df is not None else 'None'}")
    if clicks_df is not None:
        st.write(f"[DEBUG] clicks_df columns: {clicks_df.columns.tolist()}")
except Exception as e:
    st.error(f"âŒ ç‚¹å‡»æ•°æ®åŠ è½½å¤±è´¥: {e}")
    clicks_df = None

# æ£€æŸ¥æ•°æ®æ˜¯å¦éƒ½åŠ è½½æˆåŠŸ
if accounts_df is None or redash_df is None or clicks_df is None:
    st.warning("âš ï¸ éƒ¨åˆ†æ•°æ®åŠ è½½å¤±è´¥")
    
    # æ£€æŸ¥æ˜¯å¦åœ¨äº‘ç«¯ç¯å¢ƒï¼ˆé€šè¿‡æ£€æŸ¥æ˜¯å¦æœ‰secretsæ¥åˆ¤æ–­ï¼‰
    try:
        # å°è¯•è®¿é—®secretsï¼Œå¦‚æœæˆåŠŸè¯´æ˜åœ¨äº‘ç«¯
        test_secret = st.secrets.get("ACCOUNTS_URL", None)
        is_cloud = test_secret is not None
    except:
        is_cloud = False
    
    if is_cloud:
        st.error("âŒ äº‘ç«¯æ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥secretsé…ç½®å’Œæ•°æ®æ–‡ä»¶URL")
        st.stop()
    else:
        st.info("ğŸ’¡ æœ¬åœ°æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæ¼”ç¤º")
        
        # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
        import numpy as np
        from datetime import datetime, timedelta
        
        # æ¨¡æ‹Ÿ accounts æ•°æ®
        mock_accounts_data = {
            'Tiktok ID': [f'user_{i:03d}' for i in range(1, 21)],
            'Groups': ['yujie_main_avatar'] * 10 + ['wan_produce101'] * 10,
            'username': [f'user_{i:03d}' for i in range(1, 21)],
            'follower_count': np.random.randint(1000, 100000, 20),
            'like_count': np.random.randint(100, 10000, 20)
        }
        accounts_df = pd.DataFrame(mock_accounts_data)
        
        # æ¨¡æ‹Ÿ redash æ•°æ®
        dates = pd.date_range(start='2025-01-01', end='2025-01-31', freq='D')
        mock_redash_data = []
        
        for date in dates:
            for user_id in mock_accounts_data['Tiktok ID']:
                mock_redash_data.append({
                    'date': date,
                    'user_id': user_id,
                    'view_count': np.random.randint(1000, 50000),
                    'like_count': np.random.randint(100, 5000),
                    'comment_count': np.random.randint(10, 500),
                    'share_count': np.random.randint(5, 200),
                    'post_count': np.random.randint(1, 10),
                    'view_diff': np.random.randint(-1000, 5000),
                    'like_diff': np.random.randint(-100, 500),
                    'comment_diff': np.random.randint(-10, 50),
                    'share_diff': np.random.randint(-5, 20),
                    'post_diff': np.random.randint(-1, 3)
                })
        
        redash_df = pd.DataFrame(mock_redash_data)
        
        # æ¨¡æ‹Ÿ clicks æ•°æ®
        mock_clicks_data = []
        for date in dates[:10]:  # åªåˆ›å»ºå‰10å¤©çš„ç‚¹å‡»æ•°æ®
            for _ in range(np.random.randint(50, 200)):  # æ¯å¤©50-200æ¬¡ç‚¹å‡»
                mock_clicks_data.append({
                    'date': date.date(),
                    'timestamp': date,
                    'session_id': f'session_{np.random.randint(1, 1000)}',
                    'visitor_id': f'visitor_{np.random.randint(1, 500)}',
                    'page_url': np.random.choice(['https://insnap.ai/videos', 'https://insnap.ai/zh/download']),
                    'page_type': np.random.choice(['videos', 'download']),
                    'view_diff': np.random.randint(1, 100)
                })
        
        clicks_df = pd.DataFrame(mock_clicks_data)
        
        st.info("ğŸ’¡ ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæ¼”ç¤ºã€‚è¯·ç¡®ä¿æœ¬åœ°æ•°æ®æ–‡ä»¶å­˜åœ¨ï¼š")
        st.info("ğŸ“ data/postingManager_data/accounts_detail.xlsx")
        st.info("ğŸ“ data/redash_data/redash_data_2025-07-14.csv")
        st.info("ğŸ“ data/clicks/your_clicks_file.csv")

# åˆå§‹åŒ– session_state
if 'merge_successful' not in st.session_state:
    st.session_state.merge_successful = False
if 'merged_df' not in st.session_state:
    st.session_state.merged_df = None
if 'group_mapping' not in st.session_state:
    st.session_state.group_mapping = None
if 'clicks_df' not in st.session_state:
    st.session_state.clicks_df = None

# è°ƒè¯• session_state çŠ¶æ€
st.write("[DEBUG] === Session State è°ƒè¯•ä¿¡æ¯ ===")
st.write(f"[DEBUG] st.session_state.merge_successful: {st.session_state.merge_successful}")
st.write(f"[DEBUG] st.session_state.merged_df æ˜¯å¦ä¸º None: {st.session_state.merged_df is None}")
st.write(f"[DEBUG] st.session_state.group_mapping æ˜¯å¦ä¸º None: {st.session_state.group_mapping is None}")
st.write(f"[DEBUG] st.session_state.clicks_df æ˜¯å¦ä¸º None: {st.session_state.clicks_df is None}")
if st.session_state.merged_df is not None:
    st.write(f"[DEBUG] st.session_state.merged_df shape: {st.session_state.merged_df.shape}")
st.write("[DEBUG] === Session State è°ƒè¯•ä¿¡æ¯ç»“æŸ ===")

# === è‡ªåŠ¨æ£€æµ‹æ•°æ®æ–‡ä»¶å˜åŒ–ï¼Œå¿…è¦æ—¶æ¸…ç©ºåˆå¹¶ç»“æœ ===
def get_file_mtime(path):
    return os.path.getmtime(path) if os.path.exists(path) else 0

# ä½ å¯ä»¥æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´è¿™äº›è·¯å¾„
file1 = os.path.join(os.path.dirname(__file__), '..', 'data', 'posts_detail.xlsx')
file2_dir = os.path.join(os.path.dirname(__file__), '..', 'data', 'redash_data')
file2 = None
if os.path.exists(file2_dir):
    redash_files = [f for f in os.listdir(file2_dir) if f.startswith('redash_data_') and f.endswith('.csv')]
    if redash_files:
        # å–æœ€æ–°æ—¥æœŸçš„æ–‡ä»¶
        def extract_date_from_filename(filename):
            try:
                date_str = filename.replace('redash_data_', '').replace('.csv', '')
                return pd.to_datetime(date_str)
            except:
                return pd.to_datetime('1900-01-01')
        latest_file = max(redash_files, key=extract_date_from_filename)
        file2 = os.path.join(file2_dir, latest_file)

current_mtime = (get_file_mtime(file1), get_file_mtime(file2) if file2 else 0)

if "last_mtime" not in st.session_state or st.session_state["last_mtime"] != current_mtime:
    st.session_state.pop("merged_df", None)
    st.session_state.pop("merge_successful", None)
    st.session_state["last_mtime"] = current_mtime

# === é¡µé¢é¡¶éƒ¨æ·»åŠ å¼ºåˆ¶åˆ·æ–°æŒ‰é’® ===
if st.button("å¼ºåˆ¶åˆ·æ–°æ•°æ®ï¼ˆé‡æ–°åˆå¹¶ï¼‰"):
    st.session_state.pop("merged_df", None)
    st.session_state.pop("merge_successful", None)
    st.rerun()

# åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
try:
    processor = EnhancedTikTokDataProcessor(
        accounts_df=accounts_df,
        redash_df=redash_df,
        clicks_df=clicks_df
    )
    st.write("[DEBUG] EnhancedTikTokDataProcessor åˆå§‹åŒ–æˆåŠŸ")
except Exception as e:
    st.error(f"âŒ æ•°æ®å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    st.stop()

# æ£€æŸ¥æ˜¯å¦ä¹‹å‰å·²ç»æˆåŠŸåˆå¹¶
st.write("[DEBUG] æ£€æŸ¥ session_state æ¡ä»¶...")
st.write(f"[DEBUG] st.session_state.merge_successful: {st.session_state.merge_successful}")
st.write(f"[DEBUG] st.session_state.merged_df is not None: {st.session_state.merged_df is not None}")
st.write(f"[DEBUG] æ¡ä»¶ç»“æœ: {st.session_state.merge_successful and st.session_state.merged_df is not None}")

MERGED_PATH = '/tmp/merged_result.parquet'
GROUP_MAPPING_PATH = '/tmp/group_mapping.pkl'
CLICKS_PATH = '/tmp/clicks_df.parquet'

# å¯åŠ¨æ—¶ä¼˜å…ˆåŠ è½½æŒä¹…åŒ–åˆå¹¶ç»“æœ
if os.path.exists(MERGED_PATH):
    st.success("âœ… å·²åŠ è½½æŒä¹…åŒ–çš„åˆå¹¶ç»“æœï¼Œæ— éœ€é‡æ–°åˆå¹¶")
    merged_df = pd.read_parquet(MERGED_PATH)
    processor.merged_df = merged_df
    # group_mapping
    if os.path.exists(GROUP_MAPPING_PATH):
        with open(GROUP_MAPPING_PATH, 'rb') as f:
            processor.group_mapping = pickle.load(f)
    # clicks_df
    if os.path.exists(CLICKS_PATH):
        processor.clicks_df = pd.read_parquet(CLICKS_PATH)
    merge_result = True
else:
    # åˆå¹¶æ•°æ®
    try:
        st.write("[DEBUG] å¼€å§‹è°ƒç”¨ merge_data()...")
        st.write(f"[DEBUG] processor.redash_df shape: {processor.redash_df.shape if processor.redash_df is not None else 'None'}")
        st.write(f"[DEBUG] processor.accounts_df shape: {processor.accounts_df.shape if processor.accounts_df is not None else 'None'}")
        st.write(f"[DEBUG] processor.clicks_df shape: {processor.clicks_df.shape if processor.clicks_df is not None else 'None'}")
        
        # å¼ºåˆ¶åˆ·æ–°è¾“å‡º
        st.write("[DEBUG] å³å°†è°ƒç”¨ merge_data()ï¼Œè¯·ç­‰å¾…...")
        
        merge_result = processor.merge_data()
        
        st.write(f"[DEBUG] merge_data() è¿”å›å€¼: {merge_result}")
        st.write(f"[DEBUG] merge_data() å®Œæˆï¼Œmerged_df shape: {processor.merged_df.shape if processor.merged_df is not None else 'None'}")
        
        if not merge_result:
            st.error("âŒ merge_data() è¿”å› Falseï¼Œåˆå¹¶å¤±è´¥")
            st.write("[DEBUG] è¯·æ£€æŸ¥ä¸Šé¢çš„è°ƒè¯•ä¿¡æ¯ï¼Œæ‰¾å‡ºåˆå¹¶å¤±è´¥çš„å…·ä½“åŸå› ")
            
            # å°è¯•è·å–æ›´å¤šè°ƒè¯•ä¿¡æ¯
            st.write("[DEBUG] å°è¯•æ£€æŸ¥ processor çŠ¶æ€...")
            st.write(f"[DEBUG] processor.redash_df æ˜¯å¦ä¸º None: {processor.redash_df is None}")
            st.write(f"[DEBUG] processor.accounts_df æ˜¯å¦ä¸º None: {processor.accounts_df is None}")
            st.write(f"[DEBUG] processor.clicks_df æ˜¯å¦ä¸º None: {processor.clicks_df is None}")
            st.write(f"[DEBUG] processor.merged_df æ˜¯å¦ä¸º None: {processor.merged_df is None}")
            
            # æä¾›ç›´æ¥æµ‹è¯•é€‰é¡¹
            st.write("### ğŸ”§ è°ƒè¯•é€‰é¡¹")
            if st.button("è¿è¡Œç›´æ¥åˆå¹¶æµ‹è¯•"):
                st.write("### ğŸ”„ ç›´æ¥æµ‹è¯•åˆå¹¶é€»è¾‘")
                
                # è·å–æ•°æ®
                redash_df = processor.redash_df
                accounts_df = processor.accounts_df
                clicks_df = processor.clicks_df
                
                if redash_df is None or accounts_df is None:
                    st.error("âŒ æ•°æ®ä¸º Noneï¼Œæ— æ³•æµ‹è¯•")
                    st.stop()
                
                # åˆ›å»º group_mapping
                st.write("[DEBUG] åˆ›å»º group_mapping...")
                try:
                    accounts_df['Tiktok ID'] = accounts_df['Tiktok ID'].astype(str)
                    group_mapping = accounts_df[['Tiktok ID', 'Groups']].drop_duplicates()
                    group_mapping = group_mapping.rename(columns={'Tiktok ID': 'user_id', 'Groups': 'group'})
                    group_mapping['user_id'] = group_mapping['user_id'].astype(str)
                    group_mapping['group'] = group_mapping['group'].fillna('Unknown')
                    st.write(f"[DEBUG] group_mapping shape: {group_mapping.shape}")
                except Exception as e:
                    st.error(f"âŒ åˆ›å»º group_mapping å¤±è´¥: {str(e)}")
                    st.stop()
                
                # å¤„ç†æ—¥æœŸåˆ—
                st.write("[DEBUG] å¤„ç†æ—¥æœŸåˆ—...")
                try:
                    if 'date' not in redash_df.columns and 'YMDdate' in redash_df.columns:
                        redash_df['date'] = pd.to_datetime(redash_df['YMDdate'], errors='coerce')
                    redash_df = redash_df.dropna(subset=['date'])
                    st.write(f"[DEBUG] å¤„ç†æ—¥æœŸå shape: {redash_df.shape}")
                except Exception as e:
                    st.error(f"âŒ å¤„ç†æ—¥æœŸåˆ—å¤±è´¥: {str(e)}")
                    st.stop()
                
                # æ‰§è¡Œåˆå¹¶
                st.write("[DEBUG] æ‰§è¡Œåˆå¹¶...")
                try:
                    # ç¡®ä¿ user_id åˆ—çš„æ•°æ®ç±»å‹ä¸€è‡´
                    st.write(f"[DEBUG] redash_df['user_id'] dtype: {redash_df['user_id'].dtype}")
                    st.write(f"[DEBUG] group_mapping['user_id'] dtype: {group_mapping['user_id'].dtype}")
                    
                    # å°† redash_df çš„ user_id è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    redash_df['user_id'] = redash_df['user_id'].astype(str)
                    st.write(f"[DEBUG] è½¬æ¢å redash_df['user_id'] dtype: {redash_df['user_id'].dtype}")
                    
                    merged_df = redash_df.merge(group_mapping, on='user_id', how='left')
                    st.write(f"[DEBUG] åˆå¹¶æˆåŠŸï¼Œshape: {merged_df.shape}")
                    
                    # æ›´æ–° processor
                    processor.merged_df = merged_df
                    processor.group_mapping = group_mapping
                    processor.clicks_df = clicks_df
                    
                    # æŒä¹…åŒ–ä¿å­˜
                    merged_df.to_parquet('/tmp/merged_result.parquet')
                    import pickle
                    with open('/tmp/group_mapping.pkl', 'wb') as f:
                        pickle.dump(group_mapping, f)
                    if clicks_df is not None:
                        clicks_df.to_parquet('/tmp/clicks_df.parquet')

                    st.success("âœ… ç›´æ¥åˆå¹¶æµ‹è¯•æˆåŠŸï¼")
                    
                    # ä½¿ç”¨ session_state æ¥æ ‡è®°åˆå¹¶æˆåŠŸ
                    st.session_state.merge_successful = True
                    st.session_state.merged_df = merged_df
                    st.session_state.group_mapping = group_mapping
                    st.session_state.clicks_df = clicks_df
                    
                    st.success("ğŸ”„ æ•°æ®å·²æ›´æ–°ï¼Œè¯·åˆ·æ–°é¡µé¢æŸ¥çœ‹å®Œæ•´ä»ªè¡¨æ¿")
                    st.info("ğŸ’¡ æç¤ºï¼šç‚¹å‡»æµè§ˆå™¨åˆ·æ–°æŒ‰é’®æˆ–æŒ‰ F5 é”®åˆ·æ–°é¡µé¢")
                    
                    # æ˜¾ç¤ºåˆå¹¶åçš„æ•°æ®æ‘˜è¦
                    st.write("### ğŸ“Š åˆå¹¶åçš„æ•°æ®æ‘˜è¦")
                    st.write(f"- æ€»è¡Œæ•°: {len(merged_df)}")
                    st.write(f"- æ—¥æœŸèŒƒå›´: {merged_df['date'].min()} åˆ° {merged_df['date'].max()}")
                    st.write(f"- ç”¨æˆ·æ•°: {merged_df['user_id'].nunique()}")
                    st.write(f"- åˆ†ç»„æ•°: {merged_df['group'].nunique()}")
                    
                    st.stop()
            
                except Exception as e:
                    st.error(f"âŒ åˆå¹¶å¤±è´¥: {str(e)}")
                    import traceback
                    st.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            
            st.stop()
            
    except Exception as e:
        st.error(f"âŒ æ•°æ®åˆå¹¶å¤±è´¥: {e}")
        st.write(f"[DEBUG] å¼‚å¸¸è¯¦æƒ…: {str(e)}")
        import traceback
        st.error(f"[DEBUG] å®Œæ•´é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        st.stop()

if processor.merged_df is None or processor.merged_df.empty:
    st.error("âŒ æ•°æ®åˆå¹¶åä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶å†…å®¹")
    st.stop()

# è·å–æ•°æ®æ‘˜è¦
summary = processor.get_data_summary()

# è·å–æœ€æ–°ä¸€å¤©çš„å¢é‡æŒ‡æ ‡
latest_increments = processor.get_latest_day_increment_metrics()
if latest_increments:
    summary['latest_day_increments'] = latest_increments

# æ˜¾ç¤ºæ•°æ®æ‘˜è¦å¡ç‰‡
if summary:
    st.markdown("### ğŸ“Š æ•°æ®æ¦‚è§ˆ")
    summary_html = EnhancedVisualization().create_summary_cards(summary)
    st.markdown(summary_html, unsafe_allow_html=True)

# ä¾§è¾¹æ  - ç­›é€‰å™¨
st.sidebar.header("ğŸ“‹ ç­›é€‰è®¾ç½®")

# æ—¥æœŸèŒƒå›´é€‰æ‹©
if processor.merged_df is not None:
    # Debug: æ˜¾ç¤º merged_df['date'] çš„å”¯ä¸€å€¼å’Œç±»å‹
    st.sidebar.write(f"DEBUG: merged_df['date'] unique: {processor.merged_df['date'].unique()}")
    st.sidebar.write(f"DEBUG: merged_df['date'] dtype: {processor.merged_df['date'].dtype}")

    min_date = processor.merged_df['date'].min()
    max_date = processor.merged_df['date'].max()
    st.sidebar.write(f"DEBUG: min_date={min_date} ({type(min_date)}), max_date={max_date} ({type(max_date)})")
    # ä¿®å¤ NaT é—®é¢˜
    if pd.isna(min_date) or min_date is pd.NaT:
        min_date = date.today()
    if pd.isna(max_date) or max_date is pd.NaT:
        max_date = date.today()
    if isinstance(min_date, pd.Timestamp):
        min_date = min_date.date()
    if isinstance(max_date, pd.Timestamp):
        max_date = max_date.date()
    st.sidebar.write(f"DEBUG: after convert, min_date={min_date}, max_date={max_date}")
    date_range = st.sidebar.date_input(
        "é€‰æ‹©æ—¥æœŸèŒƒå›´",
        value=(min_date, max_date),
        min_value=min_date,
        max_value=max_date
    )

    if len(date_range) == 2:
        start_date, end_date = date_range
        start_date_str = start_date.strftime('%Y-%m-%d')
        end_date_str = end_date.strftime('%Y-%m-%d')
    else:
        start_date_str = None
        end_date_str = None
else:
    start_date_str = None
    end_date_str = None

# Group ç­›é€‰
available_groups = processor.get_available_groups()
selected_groups = st.sidebar.multiselect(
    "é€‰æ‹©åˆ†ç»„ (æ”¯æŒæ¨¡ç³ŠåŒ¹é…)",
    options=available_groups,
    default=available_groups[:5] if len(available_groups) > 5 else available_groups,
    help="å¯ä»¥é€‰æ‹©å¤šä¸ªåˆ†ç»„è¿›è¡Œå¯¹æ¯”åˆ†æ"
)

# ä¸»è¦å†…å®¹åŒºåŸŸ
if processor.merged_df is None or processor.merged_df.empty:
    st.warning("æ‰€é€‰ç­›é€‰æ¡ä»¶ä¸‹æ²¡æœ‰æ•°æ®")

# åˆ›å»ºæ ‡ç­¾é¡µ
tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
    "ğŸ“Š åŸºç¡€åˆ†æ",
    "ğŸ“ˆ æ¯æ—¥è¶‹åŠ¿",
    "ğŸ”— ç‚¹å‡»åˆ†æ",
    "ğŸ¯ è½¬åŒ–åˆ†æ",
    "ğŸ“Š é“¾æ¥ç‚¹å‡»é‡ & è½¬åŒ–ç‡åˆ†æ",
    "ğŸ“‹ æ•°æ®è¯¦æƒ…"
])

# åˆå§‹åŒ–å¯è§†åŒ–å·¥å…·
viz = EnhancedVisualization()

with tab1:
    st.markdown("### ğŸ“Š åŸºç¡€ä¿¡æ¯å±•ç¤º")

    # è·å–æ¯æ—¥æŒ‡æ ‡æ•°æ®
    daily_metrics = processor.get_daily_metrics(
        start_date=start_date_str,
        end_date=end_date_str,
        groups=selected_groups
    )

    if not daily_metrics.empty:
        # åˆ›å»ºä¸¤åˆ—å¸ƒå±€
        col1, col2 = st.columns(2)

        with col1:
            st.markdown('<div class="chart-container">', unsafe_allow_html=True)
            fig_views = viz.create_daily_trend_chart(
                daily_metrics, 'view_count', "æ¯æ—¥æ€»æµè§ˆé‡è¶‹åŠ¿"
            )
            st.altair_chart(fig_views, use_container_width=True)
            st.markdown('</div>', unsafe_allow_html=True)

        with col2:
            st.markdown('<div class="chart-container">', unsafe_allow_html=True)
            fig_posts = viz.create_daily_trend_chart(
                daily_metrics, 'post_count', "æ¯æ—¥æ€»å‘å¸–æ•°è¶‹åŠ¿", '#2ca02c'
            )
            st.altair_chart(fig_posts, use_container_width=True)
            st.markdown('</div>', unsafe_allow_html=True)

        # å¤šæŒ‡æ ‡å¯¹æ¯”å›¾
        st.markdown("### ğŸ“ˆ å¤šæŒ‡æ ‡å¯¹æ¯”")
        required_columns = ['view_count', 'like_count', 'comment_count', 'share_count']
        if daily_metrics is not None and not daily_metrics.empty:
            missing_columns = [col for col in required_columns if col not in daily_metrics.columns]
            if missing_columns:
                st.warning(f"ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_columns}")
                st.write("å¯ç”¨çš„åˆ—:", list(daily_metrics.columns))
            else:
                metrics_comparison = viz.create_multi_metric_comparison(
                    daily_metrics,
                    required_columns,
                    "æ¯æ—¥æŒ‡æ ‡å¯¹æ¯”"
                )
                st.altair_chart(metrics_comparison, use_container_width=True)
        else:
            st.warning("daily_metrics æ•°æ®ä¸ºç©ºæˆ–ä¸å­˜åœ¨")

    # è¡¨ç°æœ€å¥½è´¦å·Top 5æ¿å—
    st.markdown("### ğŸ“Š è¡¨ç°æœ€å¥½è´¦å· Top 5ï¼ˆæŒ‰æœ€åä¸€å¤©æ–°å¢æµè§ˆé‡ï¼‰")

    # è·å–è¡¨ç°æœ€å¥½çš„è´¦å·
    top_accounts = processor.get_top_performing_accounts(
        start_date=start_date_str,
        end_date=end_date_str,
        top_n=5
    )

    if not top_accounts.empty:
        # æ ¼å¼åŒ–æ•°æ®ç”¨äºæ˜¾ç¤º
        display_df = top_accounts.copy()

        # åˆ›å»ºå¯ç‚¹å‡»çš„ç”¨æˆ·ååˆ—
        display_df['ç”¨æˆ·å'] = display_df.apply(
            lambda row: f"[{row['username']}]({row['profile_url']})" if row['username'] != 'æœªçŸ¥' and row['profile_url'] else row['username'],
            axis=1
        )

        # æ ¼å¼åŒ–æ•°å­—å­—æ®µ
        for col in ['last_day_view_increment', 'follower_count', 'like_count']:
            if col in display_df.columns:
                # å…ˆè½¬æ¢ä¸ºæ•°å€¼ï¼Œå¤„ç†NaNï¼Œç„¶åæ ¼å¼åŒ–
                numeric_series = pd.to_numeric(display_df[col], errors='coerce')
                numeric_series = numeric_series.fillna(0)
                display_df[col] = numeric_series.astype(int).apply(lambda x: f"{x:,}")

        # é‡å‘½ååˆ—ç”¨äºæ˜¾ç¤º
        display_df = display_df.rename(columns={
            'user_id': 'è´¦å· ID',
            'last_day_view_increment': 'æœ€åä¸€å¤©æ–°å¢æµè§ˆé‡',
            'follower_count': 'æ€»ç²‰ä¸æ•°',
            'like_count': 'ç‚¹èµæ•°'
        })

        # é€‰æ‹©æ˜¾ç¤ºçš„åˆ—
        display_columns = ['ç”¨æˆ·å', 'è´¦å· ID', 'æœ€åä¸€å¤©æ–°å¢æµè§ˆé‡', 'æ€»ç²‰ä¸æ•°', 'ç‚¹èµæ•°']
        display_df = display_df[display_columns]

        # æ˜¾ç¤ºè¡¨æ ¼
        st.markdown("""
        <style>
        .dataframe {
            font-size: 14px;
        }
        .dataframe th {
            background-color: #f0f2f6;
            font-weight: bold;
            text-align: center;
        }
        .dataframe td {
            text-align: center;
        }
        </style>
        """, unsafe_allow_html=True)

        # ä½¿ç”¨st.markdownæ˜¾ç¤ºè¡¨æ ¼ï¼Œæ”¯æŒé“¾æ¥ç‚¹å‡»
        st.markdown("#### ğŸ“‹ è´¦å·è¡¨ç°æ’å")

        # åˆ›å»ºè¡¨æ ¼å¤´éƒ¨
        table_header = "| ç”¨æˆ·å | è´¦å· ID | æœ€åä¸€å¤©æ–°å¢æµè§ˆé‡ | æ€»ç²‰ä¸æ•° | ç‚¹èµæ•° |\n"
        table_separator = "|--------|---------|-------------------|----------|--------|\n"

        # åˆ›å»ºè¡¨æ ¼å†…å®¹
        table_rows = []
        for _, row in display_df.iterrows():
            username = row['ç”¨æˆ·å']
            user_id = row['è´¦å· ID']
            view_increment = row['æœ€åä¸€å¤©æ–°å¢æµè§ˆé‡']
            followers = row['æ€»ç²‰ä¸æ•°']
            likes = row['ç‚¹èµæ•°']
            table_rows.append(f"| {username} | {user_id} | {view_increment} | {followers} | {likes} |")

        # ç»„åˆå®Œæ•´è¡¨æ ¼
        full_table = table_header + table_separator + "\n".join(table_rows)

        # æ˜¾ç¤ºè¡¨æ ¼
        st.markdown(full_table)

        # æ·»åŠ è¯´æ˜
        st.markdown("""
        <div style='background-color: #f8f9fa; padding: 10px; border-radius: 5px; margin-top: 10px;'>
        <small>
        ğŸ“ <strong>è¯´æ˜ï¼š</strong><br>
        â€¢ æœ€åä¸€å¤©æ–°å¢æµè§ˆé‡ï¼šåŸºäºé€‰å®šæ—¥æœŸèŒƒå›´å†…æœ€åä¸€å¤©çš„ view_diff å­—æ®µ<br>
        â€¢ æ€»ç²‰ä¸æ•°/ç‚¹èµæ•°ï¼šæ¥è‡ª accounts_detail è¡¨çš„æœ€æ–°æ•°æ®<br>
        â€¢ ç”¨æˆ·åï¼šç‚¹å‡»å¯è·³è½¬åˆ° TikTok ä¸»é¡µ<br>
        â€¢ æ•°æ®èŒƒå›´ï¼šå½“å‰ç­›é€‰çš„æ—¥æœŸèŒƒå›´
        </small>
        </div>
        """, unsafe_allow_html=True)

    else:
        st.info("æš‚æ— è¡¨ç°æ•°æ®æˆ–æ•°æ®ä¸è¶³")

with tab2:
    st.markdown("## ğŸ“Š æ¯æ—¥æ–°å¢è¶‹åŠ¿åˆ†æ")

    # æ¨¡å¼åˆ‡æ¢
    mode = st.radio("å±•ç¤ºæ¨¡å¼", ["æ‰€æœ‰è´¦å·", "åˆ†ç»„æ¨¡å¼"], horizontal=True)

    # å®šä¹‰æŒ‡æ ‡é…ç½®
    metric_configs = [
        ("view_count_inc", "æ–°å¢æµè§ˆé‡", "#1f77b4", "æ¬¡"),
        ("like_count_inc", "æ–°å¢ç‚¹èµé‡", "#ff7f0e", "ä¸ª"),
        ("comment_count_inc", "æ–°å¢è¯„è®ºæ•°", "#2ca02c", "æ¡"),
        ("share_count_inc", "æ–°å¢åˆ†äº«æ•°", "#d62728", "æ¬¡"),
        ("post_count_inc", "æ–°å¢å‘å¸–æ•°", "#9467bd", "æ¡")
    ]

    def create_increment_chart(data, metric_col, title, color, unit):
        """åˆ›å»ºå•ä¸ªæ–°å¢æŒ‡æ ‡å›¾è¡¨"""
        import altair as alt

        # è®¡ç®—æ•°æ®èŒƒå›´ï¼Œè®¾ç½®è‡ªé€‚åº”çºµè½´
        min_value = data[metric_col].min()
        max_value = data[metric_col].max()

        # è®¾ç½®çºµè½´èŒƒå›´ï¼Œé¿å…ä»0å¼€å§‹ï¼Œå¢å¼ºè¶‹åŠ¿å¯è¯»æ€§
        if min_value != max_value:
            data_range = max_value - min_value
            y_min = max(0, min_value - data_range * 0.05)  # æœ€å°ä¸ä½äº0
            y_max = max_value + data_range * 0.05
        else:
            y_min = max(0, min_value * 0.95)
            y_max = max_value * 1.05

        chart = alt.Chart(data).mark_line(point=True, color=color).encode(
            x=alt.X('date:T', title='æ—¥æœŸ'),
            y=alt.Y(f'{metric_col}:Q',
                   title=f'æ¯æ—¥æ–°å¢ ({unit})',
                   scale=alt.Scale(domain=[y_min, y_max], zero=False)),  # è®¾ç½®è‡ªé€‚åº”èŒƒå›´ï¼Œä¸ä»0å¼€å§‹
            tooltip=[
                alt.Tooltip('date:T', title='æ—¥æœŸ', format='%Y-%m-%d'),
                alt.Tooltip(f'{metric_col}:Q', title=title, format=',.0f')
            ]
        ).properties(
            title=f"ğŸ“ˆ {title}è¶‹åŠ¿",
            height=300
        )
        return chart

    if mode == "æ‰€æœ‰è´¦å·":
        inc_df = processor.get_daily_increment_metrics(start_date_str, end_date_str)
        if not inc_df.empty:
            st.markdown("### ğŸ“Š æ‰€æœ‰è´¦å·æ¯æ—¥æ–°å¢æŒ‡æ ‡")
            st.markdown("*ä»¥ä¸‹å›¾è¡¨å±•ç¤ºåŸºäº redash_data è®¡ç®—çš„æ¯æ—¥æ–°å¢æ•°æ®*")

            # åˆ›å»ºä¸¤åˆ—å¸ƒå±€å±•ç¤ºå›¾è¡¨
            for i in range(0, len(metric_configs), 2):
                col1, col2 = st.columns(2)

                with col1:
                    metric_col, title, color, unit = metric_configs[i]
                    chart = create_increment_chart(inc_df, metric_col, title, color, unit)
                    st.altair_chart(chart, use_container_width=True)

                # å¦‚æœè¿˜æœ‰ä¸‹ä¸€ä¸ªæŒ‡æ ‡ï¼Œåœ¨ç¬¬äºŒåˆ—æ˜¾ç¤º
                if i + 1 < len(metric_configs):
                    with col2:
                        metric_col, title, color, unit = metric_configs[i + 1]
                        chart = create_increment_chart(inc_df, metric_col, title, color, unit)
                        st.altair_chart(chart, use_container_width=True)

            # å¦‚æœæŒ‡æ ‡æ•°é‡æ˜¯å¥‡æ•°ï¼Œæœ€åä¸€ä¸ªæŒ‡æ ‡å•ç‹¬å ä¸€è¡Œ
            if len(metric_configs) % 2 == 1:
                metric_col, title, color, unit = metric_configs[-1]
                chart = create_increment_chart(inc_df, metric_col, title, color, unit)
                st.altair_chart(chart, use_container_width=True)

        else:
            st.info("æš‚æ— æ•°æ®")

    else:
        # åˆ†ç»„æ¨¡å¼
        all_groups = processor.get_available_groups()
        selected_groups = st.multiselect(
            "é€‰æ‹©åˆ†ç»„å…³é”®è¯ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰",
            options=all_groups,
            default=all_groups[:2] if len(all_groups) > 2 else all_groups,
            help="é€‰æ‹©åå°†æ‰€æœ‰åŒ…å«è¯¥å…³é”®è¯çš„è´¦å·å½’ä¸ºä¸€ç»„ï¼Œæ¯å¼ å›¾æ˜¾ç¤ºè¯¥ç»„çš„åˆå¹¶è¡¨ç°"
        )

        if selected_groups:
            # è·å–å½“å‰é€‰ä¸­åˆ†ç»„çš„æœ€æ–°ä¸€å¤©å¢é‡æŒ‡æ ‡
            latest_increments = processor.get_selected_groups_latest_increments(selected_groups)

            if latest_increments:
                st.markdown("### ğŸ“Š åˆ†ç»„æ¯æ—¥æ–°å¢æŒ‡æ ‡")
                st.markdown("*ä»¥ä¸‹å›¾è¡¨å±•ç¤ºåŸºäº redash_data è®¡ç®—çš„æ¯æ—¥æ–°å¢æ•°æ®ï¼ŒæŒ‰åˆ†ç»„å…³é”®è¯èšåˆ*")

                # æ˜¾ç¤ºåˆ†ç»„å¢é‡æŒ‡æ ‡æ‘˜è¦å¡ç‰‡
                st.markdown("#### ğŸ“ˆ ä»Šæ—¥åˆ†ç»„å¢é‡æŒ‡æ ‡æ‘˜è¦")

                # ä¸ºæ¯ä¸ªåˆ†ç»„æ˜¾ç¤ºæ‘˜è¦å¡ç‰‡
                for group_keyword, metrics in latest_increments.items():
                    st.markdown(f"**ğŸ¯ åˆ†ç»„: {group_keyword}** ({metrics['date']})")

                    # åˆ›å»º5åˆ—å¸ƒå±€æ˜¾ç¤ºå„é¡¹æŒ‡æ ‡
                    col1, col2, col3, col4, col5 = st.columns(5)

                    with col1:
                        st.metric("ğŸ“„ æ–°å¢å‘å¸–é‡", f"{metrics['posts']:,}")

                    with col2:
                        st.metric("ğŸ‘€ æ–°å¢æµè§ˆé‡", f"{metrics['views']:,}")

                    with col3:
                        st.metric("ğŸ‘ æ–°å¢ç‚¹èµæ•°", f"{metrics['likes']:,}")

                    with col4:
                        st.metric("ğŸ’¬ æ–°å¢è¯„è®ºæ•°", f"{metrics['comments']:,}")

                    with col5:
                        st.metric("ğŸ”„ æ–°å¢åˆ†äº«æ•°", f"{metrics['shares']:,}")

                    st.markdown("---")  # åˆ†éš”çº¿

                # è·å–åˆ†ç»„æ¯æ—¥å¢é‡æ•°æ®ç”¨äºå›¾è¡¨
                inc_df = processor.get_group_daily_increment_metrics(start_date_str, end_date_str, selected_groups)
            else:
                st.markdown("### ğŸ“Š åˆ†ç»„æ¯æ—¥æ–°å¢æŒ‡æ ‡")
                st.markdown("*ä»¥ä¸‹å›¾è¡¨å±•ç¤ºåŸºäº redash_data è®¡ç®—çš„æ¯æ—¥æ–°å¢æ•°æ®ï¼ŒæŒ‰åˆ†ç»„å…³é”®è¯èšåˆ*")
                st.info("âš ï¸ æš‚æ— åˆ†ç»„å¢é‡æŒ‡æ ‡æ•°æ®")
                inc_df = processor.get_group_daily_increment_metrics(start_date_str, end_date_str, selected_groups)

            if not inc_df.empty:
                # ä¸ºæ¯ä¸ªåˆ†ç»„å…³é”®è¯åˆ›å»ºå›¾è¡¨
                for group_keyword in selected_groups:
                    st.markdown(f"#### ğŸ¯ åˆ†ç»„: {group_keyword}")

                    # ç­›é€‰è¯¥åˆ†ç»„çš„æ•°æ®
                    group_data = inc_df[inc_df['group'] == group_keyword].copy()
                    if not group_data.empty:
                        # åˆ›å»ºä¸¤åˆ—å¸ƒå±€å±•ç¤ºå›¾è¡¨
                        for i in range(0, len(metric_configs), 2):
                            col1, col2 = st.columns(2)

                            with col1:
                                metric_col, title, color, unit = metric_configs[i]
                                chart = create_increment_chart(group_data, metric_col, f"{group_keyword} - {title}", color, unit)
                                st.altair_chart(chart, use_container_width=True)

                            # å¦‚æœè¿˜æœ‰ä¸‹ä¸€ä¸ªæŒ‡æ ‡ï¼Œåœ¨ç¬¬äºŒåˆ—æ˜¾ç¤º
                            if i + 1 < len(metric_configs):
                                with col2:
                                    metric_col, title, color, unit = metric_configs[i + 1]
                                    chart = create_increment_chart(group_data, metric_col, f"{group_keyword} - {title}", color, unit)
                                    st.altair_chart(chart, use_container_width=True)
                    else:
                        st.warning(f"åˆ†ç»„ '{group_keyword}' æš‚æ— æ•°æ®")

                    st.markdown("---")  # åˆ†éš”çº¿
            else:
                st.info("æ— æ•°æ®æˆ–æ‰€é€‰åˆ†ç»„æ— æ•°æ®")
        else:
            st.info("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªåˆ†ç»„å…³é”®è¯")

with tab3:
    st.markdown("### ğŸ”— ç‚¹å‡»é‡åˆ†æ")

    if processor.clicks_df is not None:
        # è·å–ç‚¹å‡»é‡æŒ‡æ ‡
        clicks_metrics = processor.get_clicks_metrics(
            start_date=start_date_str,
            end_date=end_date_str
        )

        if clicks_metrics and 'daily_clicks' in clicks_metrics:
            daily_clicks = clicks_metrics['daily_clicks']

            if not daily_clicks.empty:
                # æ£€æŸ¥æ•°æ®æ ¼å¼ï¼Œç¡®ä¿åŒ…å«å¿…è¦çš„åˆ—
                required_cols = ['date', 'daily_clicks', 'daily_visitors']
                available_cols = daily_clicks.columns.tolist()
                
                # å¦‚æœç¼ºå°‘å¿…è¦çš„åˆ—ï¼Œå°è¯•ä»å…¶ä»–æ•°æ®æºè·å–æˆ–åˆ›å»º
                if not all(col in available_cols for col in required_cols):
                    st.warning("âš ï¸ ç‚¹å‡»æ•°æ®æ ¼å¼ä¸å®Œæ•´ï¼Œæ­£åœ¨å°è¯•ä¿®å¤...")
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–å¯ç”¨çš„åˆ—
                    if 'date' in available_cols:
                        # åˆ›å»ºä¸´æ—¶çš„ç‚¹å‡»é‡æ•°æ®
                        fixed_data = daily_clicks.copy()
                        
                        # å¦‚æœç¼ºå°‘ daily_clicks åˆ—ï¼Œå°è¯•ä»å…¶ä»–åˆ—æ¨æ–­æˆ–ä½¿ç”¨é»˜è®¤å€¼
                        if 'daily_clicks' not in available_cols:
                            if 'clicks_count' in available_cols:
                                fixed_data['daily_clicks'] = fixed_data['clicks_count']
                            elif 'pv' in available_cols:
                                fixed_data['daily_clicks'] = fixed_data['pv']
                            else:
                                # ä½¿ç”¨é»˜è®¤å€¼
                                fixed_data['daily_clicks'] = 0
                        
                        # å¦‚æœç¼ºå°‘ daily_visitors åˆ—ï¼Œå°è¯•ä»å…¶ä»–åˆ—æ¨æ–­æˆ–ä½¿ç”¨é»˜è®¤å€¼
                        if 'daily_visitors' not in available_cols:
                            if 'visitors_count' in available_cols:
                                fixed_data['daily_visitors'] = fixed_data['visitors_count']
                            elif 'uv' in available_cols:
                                fixed_data['daily_visitors'] = fixed_data['uv']
                            else:
                                # ä½¿ç”¨é»˜è®¤å€¼
                                fixed_data['daily_visitors'] = 0
                        
                        daily_clicks = fixed_data
                        st.success("âœ… æ•°æ®æ ¼å¼å·²ä¿®å¤")
                    else:
                        st.error("âŒ æ— æ³•ä¿®å¤æ•°æ®æ ¼å¼ï¼Œç¼ºå°‘å¿…è¦çš„åˆ—")
                        st.stop()
                
                # ç‚¹å‡»é‡è¶‹åŠ¿å›¾
                st.markdown("#### ğŸ“ˆ æ¯æ—¥ç‚¹å‡»é‡è¶‹åŠ¿")
                try:
                    clicks_chart = viz.create_clicks_analysis_chart(
                        daily_clicks, "æ¯æ—¥ç‚¹å‡»é‡è¶‹åŠ¿"
                    )
                    st.altair_chart(clicks_chart, use_container_width=True)
                except Exception as e:
                    st.error(f"âŒ åˆ›å»ºç‚¹å‡»é‡è¶‹åŠ¿å›¾å¤±è´¥: {str(e)}")
                    st.write("è°ƒè¯•ä¿¡æ¯ï¼š")
                    st.write(f"æ•°æ®åˆ—: {daily_clicks.columns.tolist()}")
                    st.write(f"æ•°æ®å½¢çŠ¶: {daily_clicks.shape}")
                    st.write(f"æ•°æ®å‰5è¡Œ: {daily_clicks.head()}")

                # ç‚¹å‡»é‡ç»Ÿè®¡
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("æ€»ç‚¹å‡»é‡", f"{clicks_metrics['total_clicks']:,}")
                with col2:
                    st.metric("ç»Ÿè®¡å¤©æ•°", f"{clicks_metrics['unique_dates']}")
                with col3:
                    avg_clicks = clicks_metrics['total_clicks'] / clicks_metrics['unique_dates'] if clicks_metrics['unique_dates'] > 0 else 0
                    st.metric("æ—¥å‡ç‚¹å‡»é‡", f"{avg_clicks:.0f}")

                # é¡µé¢ç±»å‹ç‚¹å‡»ç»Ÿè®¡
                if 'page_type_clicks' in clicks_metrics and not clicks_metrics['page_type_clicks'].empty:
                    st.markdown("#### ğŸ“Š é¡µé¢ç±»å‹ç‚¹å‡»ç»Ÿè®¡")
                    page_type_data = clicks_metrics['page_type_clicks']
                    st.dataframe(page_type_data, use_container_width=True)

                # é“¾æ¥ç‚¹å‡»ç»Ÿè®¡
                if 'link_clicks' in clicks_metrics and not clicks_metrics['link_clicks'].empty:
                    st.markdown("#### ğŸ”— é“¾æ¥ç‚¹å‡»ç»Ÿè®¡")
                    link_data = clicks_metrics['link_clicks'].head(10)
                    st.dataframe(link_data, use_container_width=True)
    else:
        st.warning("æš‚æ— ç‚¹å‡»æ•°æ®")

with tab4:
    st.markdown("### ğŸ¯ è½¬åŒ–åˆ†æ")

    if processor.clicks_df is not None and processor.merged_df is not None:
        # è·å–æœ€åä¸€å¤©çš„ç‚¹å‡»ç»Ÿè®¡æ‘˜è¦
        last_day_summary = processor.get_last_day_clicks_summary(
            start_date=start_date_str,
            end_date=end_date_str
        )

        if last_day_summary:
            st.markdown("### ğŸ“Š æœ€åä¸€å¤©ç‚¹å‡»æƒ…å†µæ‘˜è¦")

            # æ˜¾ç¤ºæœ€åä¸€å¤©æ—¥æœŸ
            sample_link = list(last_day_summary.keys())[0]
            last_day_date = last_day_summary[sample_link]['date']
            st.markdown(f"**ç»Ÿè®¡æ—¥æœŸï¼š{last_day_date}**")

            # åˆ›å»ºä¸¤åˆ—å¸ƒå±€ï¼Œåˆ†åˆ«æ˜¾ç¤ºä¸¤ä¸ªé“¾æ¥çš„ç»Ÿè®¡å¡ç‰‡
            col1, col2 = st.columns(2)

            with col1:
                st.markdown("#### ğŸ”— https://insnap.ai/videos")
                if 'https://insnap.ai/videos' in last_day_summary:
                    data = last_day_summary['https://insnap.ai/videos']
                    # æ·»åŠ å­—æ®µæ£€æŸ¥ï¼Œé¿å…KeyError
                    pv_value = data.get('pv', 0)
                    uv_value = data.get('uv', 0)
                    st.metric("ä»Šæ—¥ç‚¹å‡»é‡(PV)", f"{pv_value:,}")
                    st.metric("ä»Šæ—¥è®¿å®¢æ•°(UV)", f"{uv_value:,}")
                else:
                    st.metric("ä»Šæ—¥ç‚¹å‡»é‡(PV)", "0")
                    st.metric("ä»Šæ—¥è®¿å®¢æ•°(UV)", "0")

            with col2:
                st.markdown("#### ğŸ”— https://insnap.ai/zh/download")
                if 'https://insnap.ai/zh/download' in last_day_summary:
                    data = last_day_summary['https://insnap.ai/zh/download']
                    # æ·»åŠ å­—æ®µæ£€æŸ¥ï¼Œé¿å…KeyError
                    pv_value = data.get('pv', 0)
                    uv_value = data.get('uv', 0)
                    st.metric("ä»Šæ—¥ç‚¹å‡»é‡(PV)", f"{pv_value:,}")
                    st.metric("ä»Šæ—¥è®¿å®¢æ•°(UV)", f"{uv_value:,}")
                else:
                    st.metric("ä»Šæ—¥ç‚¹å‡»é‡(PV)", "0")
                    st.metric("ä»Šæ—¥è®¿å®¢æ•°(UV)", "0")

            # æ·»åŠ è¯´æ˜
            st.markdown("""
            <div style='background-color: #f8f9fa; padding: 10px; border-radius: 5px; margin: 10px 0;'>
            <small>
            ğŸ“ <strong>è¯´æ˜ï¼š</strong><br>
            â€¢ ä»Šæ—¥ç‚¹å‡»é‡(PV)ï¼šåŸºäºä¸åŒ session_id ç»Ÿè®¡çš„ç‚¹å‡»æ¬¡æ•°<br>
            â€¢ ä»Šæ—¥è®¿å®¢æ•°(UV)ï¼šåŸºäºä¸åŒ visitor_id ç»Ÿè®¡çš„ç‹¬ç«‹è®¿å®¢æ•°<br>
            â€¢ æ•°æ®æ¥æºï¼šé“¾æ¥ç‚¹å‡»æ•°æ®è¡¨ï¼Œç»Ÿè®¡å½“å‰é€‰æ‹©æ—¥æœŸèŒƒå›´çš„æœ€åä¸€å¤©
            </small>
            </div>
            """, unsafe_allow_html=True)

            st.markdown("---")  # åˆ†éš”çº¿

            # è·å–è½¬åŒ–åˆ†ææ•°æ®
            conversion_data = processor.get_clicks_conversion_analysis(
                start_date=start_date_str,
                end_date=end_date_str
            )

            if not conversion_data.empty:
                # è½¬åŒ–ç‡å›¾è¡¨
                st.markdown("#### ğŸ“ˆ ç‚¹å‡»åˆ°æµè§ˆé‡è½¬åŒ–ç‡")
                conversion_chart = viz.create_conversion_chart(
                    conversion_data, 'clicks_count', 'view_count', "ç‚¹å‡»åˆ°æµè§ˆé‡è½¬åŒ–ç‡"
                )
                st.altair_chart(conversion_chart, use_container_width=True)

                # åˆ†ç»„ç‚¹å‡»åˆ†æ
                group_clicks_data = processor.get_group_clicks_analysis(
                    start_date=start_date_str,
                    end_date=end_date_str
                )

                if not group_clicks_data.empty:
                    st.markdown("#### ğŸ“Š åˆ†ç»„ç‚¹å‡»ç›¸å…³æ€§åˆ†æ")

                    # é€‰æ‹©æŒ‡æ ‡è¿›è¡Œç›¸å…³æ€§åˆ†æ
                    metric_options = ['view_count', 'like_count', 'comment_count', 'share_count']
                    selected_corr_metric = st.selectbox(
                        "é€‰æ‹©ç›¸å…³æ€§åˆ†ææŒ‡æ ‡",
                        options=metric_options,
                        format_func=lambda x: x.replace('_', ' ').title()
                    )

                    correlation_chart = viz.create_group_clicks_correlation(
                        group_clicks_data, 'link_clicks', selected_corr_metric,
                        f"ç‚¹å‡»é‡ä¸{selected_corr_metric.replace('_', ' ').title()}ç›¸å…³æ€§"
                    )
                    st.altair_chart(correlation_chart, use_container_width=True)

                    # æ˜¾ç¤ºåˆ†ç»„ç‚¹å‡»æ•°æ®
                    st.markdown("#### ğŸ“‹ åˆ†ç»„ç‚¹å‡»æ•°æ®è¯¦æƒ…")
                    st.dataframe(group_clicks_data, use_container_width=True)
        else:
            st.warning("æš‚æ— è½¬åŒ–åˆ†ææ•°æ®")

with tab5:
    st.markdown("## ğŸ“Š é“¾æ¥ç‚¹å‡»é‡ & è½¬åŒ–ç‡åˆ†æ")

    if processor.clicks_df is not None and processor.merged_df is not None:
        # è·å–é“¾æ¥è½¬åŒ–ç‡åˆ†ææ•°æ®
        link_conversion_data = processor.get_link_conversion_analysis(
            start_date=start_date_str,
            end_date=end_date_str
        )

        if link_conversion_data:
            st.markdown("### ğŸ”— é“¾æ¥æ˜ å°„å…³ç³»")
            st.markdown("""
            - `https://insnap.ai/videos` â†’ ç›®æ ‡åˆ†ç»„ï¼š`yujie_main_avatar`
            - `https://insnap.ai/zh/download` â†’ ç›®æ ‡åˆ†ç»„ï¼š`wan_produce101`
            """)

            # ä¸ºæ¯ä¸ªé“¾æ¥åˆ›å»ºåˆ†æå›¾è¡¨
            for link_url, analysis_data in link_conversion_data.items():
                st.markdown(f"#### ğŸ¯ {link_url}")
                st.markdown(f"*ç›®æ ‡åˆ†ç»„: {analysis_data['target_group']}*")

                # ç¡®ä¿dataä¸ºDataFrame
                data = analysis_data['data'].copy()
                required_cols = ['date', 'daily_clicks', 'daily_visitors', 'daily_views']
                if not isinstance(data, pd.DataFrame) or data.empty:
                    st.error('æ•°æ®ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®ï¼Œè¯·æ£€æŸ¥æ•°æ®æºã€‚')
                    st.stop()
                missing_cols = [col for col in required_cols if col not in data.columns]
                if missing_cols:
                    st.error(f'æ•°æ®ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_cols}')
                    st.stop()

                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯å¡ç‰‡
                col1, col2, col3, col4, col5 = st.columns(5)
                with col1:
                    st.metric("æ€»ç‚¹å‡»é‡(PV)", f"{analysis_data['total_clicks']:,}")
                with col2:
                    st.metric("æ€»è®¿å®¢æ•°(UV)", f"{analysis_data['total_visitors']:,}")
                with col3:
                    st.metric("æ€»æµè§ˆé‡", f"{analysis_data['total_views']:,}")
                with col4:
                    st.metric("PVè½¬åŒ–ç‡", f"{analysis_data['avg_pv_conversion_rate']:.2f}%")
                with col5:
                    st.metric("UVè½¬åŒ–ç‡", f"{analysis_data['avg_uv_conversion_rate']:.2f}%")

                # æ–°å¢ï¼šä»Šæ—¥æ–°å¢å¡ç‰‡
                today = analysis_data.get('today', {})
                st.markdown(f"#### ğŸ“… ä»Šæ—¥æ–°å¢ï¼ˆ{today.get('date', '')}ï¼‰")
                col1, col2, col3, col4, col5 = st.columns(5)
                with col1:
                    st.metric("ä»Šæ—¥æ–°å¢ç‚¹å‡»é‡(PV)", f"{today.get('pv', 0):,}")
                with col2:
                    st.metric("ä»Šæ—¥æ–°å¢è®¿å®¢æ•°(UV)", f"{today.get('uv', 0):,}")
                with col3:
                    st.metric("ä»Šæ—¥æ–°å¢æµè§ˆé‡", f"{today.get('views', 0):,}")
                with col4:
                    st.metric("ä»Šæ—¥PVè½¬åŒ–ç‡", f"{today.get('pv_rate', 0.0):.2f}%")
                with col5:
                    st.metric("ä»Šæ—¥UVè½¬åŒ–ç‡", f"{today.get('uv_rate', 0.0):.2f}%")

                # è°ƒè¯•è¾“å‡º
                with st.expander("ğŸ” è°ƒè¯•æ•°æ®"):
                    st.write("åŸå§‹æ•°æ®æ ·æœ¬:")
                    st.dataframe(analysis_data['data'].head(), use_container_width=True)
                    st.write(f"æ•°æ®å½¢çŠ¶: {analysis_data['data'].shape}")
                    st.write(f"åˆ—å: {list(analysis_data['data'].columns)}")

                # å›¾è¡¨æ˜¾ç¤ºé€‰æ‹©
                st.markdown("### ğŸ“ˆ è¶‹åŠ¿å›¾è¡¨")
                chart_options = st.multiselect(
                    "é€‰æ‹©è¦æ˜¾ç¤ºçš„æŒ‡æ ‡",
                    options=["ç‚¹å‡»é‡(PV)", "è®¿å®¢æ•°(UV)", "æµè§ˆé‡", "PVè½¬åŒ–ç‡", "UVè½¬åŒ–ç‡"],
                    default=["ç‚¹å‡»é‡(PV)", "è®¿å®¢æ•°(UV)", "æµè§ˆé‡"],
                    key=f"chart_options_{link_url}"
                )

                # åˆ›å»ºåŒè½´å›¾è¡¨ï¼šPVå’ŒUVå¯¹æ¯”
                import altair as alt

                # æ„å»ºPV/UVè¶‹åŠ¿å›¾æ‰€éœ€æ•°æ®
                required_cols = ['date', 'daily_clicks']
                missing_cols = [col for col in required_cols if col not in data.columns]
                if missing_cols:
                    st.error(f'æ•°æ®ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_cols}')
                    st.stop()
                pv_data = data[['date', 'daily_clicks']].copy()
                pv_data['type'] = 'ç‚¹å‡»é‡(PV)'
                pv_data['value'] = pv_data['daily_clicks']
                uv_data = data[['date', 'daily_visitors']].copy()
                uv_data['type'] = 'è®¿å®¢æ•°(UV)'
                uv_data['value'] = uv_data['daily_visitors']
                combined_data = pd.concat([pv_data, uv_data], ignore_index=True)
                base = alt.Chart(combined_data).encode(
                    x=alt.X('date:T', title='æ—¥æœŸ', axis=alt.Axis(format='%Y-%m-%d'))
                )

                # åªä¿ç•™PV/UVä¸¤çº¿è¶‹åŠ¿å›¾
                pv_chart = base.transform_filter(alt.datum.type == 'ç‚¹å‡»é‡(PV)').mark_line(
                    color='#1f77b4', point=True, strokeWidth=2
                ).encode(
                    y=alt.Y('value:Q', title='ç‚¹å‡»é‡/è®¿å®¢æ•°',
                            scale=alt.Scale(domain=[0, max(1, combined_data[combined_data['type'].isin(['ç‚¹å‡»é‡(PV)','è®¿å®¢æ•°(UV)'])]['value'].max() * 1.1)]),
                            axis=alt.Axis(format=',')),
                    tooltip=[
                        alt.Tooltip('date:T', title='æ—¥æœŸ', format='%Y-%m-%d'),
                        alt.Tooltip('value:Q', title='ç‚¹å‡»é‡(PV)', format=',.0f')
                    ]
                )
                uv_chart = base.transform_filter(alt.datum.type == 'è®¿å®¢æ•°(UV)').mark_line(
                    color='#ff7f0e', point=True, strokeWidth=2
                ).encode(
                    y=alt.Y('value:Q', title='ç‚¹å‡»é‡/è®¿å®¢æ•°',
                            scale=alt.Scale(domain=[0, max(1, combined_data[combined_data['type'].isin(['ç‚¹å‡»é‡(PV)','è®¿å®¢æ•°(UV)'])]['value'].max() * 1.1)]),
                            axis=alt.Axis(format=',')),
                    tooltip=[
                        alt.Tooltip('date:T', title='æ—¥æœŸ', format='%Y-%m-%d'),
                        alt.Tooltip('value:Q', title='è®¿å®¢æ•°(UV)', format=',.0f')
                    ]
                )

                chart = alt.layer(pv_chart, uv_chart).resolve_scale(
                    y='independent'
                ).properties(
                    title=f"ğŸ“ˆ {link_url} - PV vs UV è¶‹åŠ¿å¯¹æ¯”",
                    height=400
                ).configure_axis(
                    gridColor='#f0f0f0'
                ).configure_view(
                    strokeWidth=0
                )

                st.altair_chart(chart, use_container_width=True)

                # æ¯æ—¥æ•°æ®è¡¨æ ¼
                st.markdown("#### ğŸ“‹ æ¯æ—¥æ•°æ®æ˜ç»†")

                # å‡†å¤‡è¡¨æ ¼æ•°æ®
                table_data = data[['date', 'daily_clicks', 'daily_visitors', 'daily_views']].copy()
                table_data = table_data.rename(columns={
                    'date': 'æ—¥æœŸ',
                    'daily_clicks': 'ç‚¹å‡»é‡(PV)',
                    'daily_visitors': 'è®¿å®¢æ•°(UV)',
                    'daily_views': 'æµè§ˆé‡'
                })

                # æ ¼å¼åŒ–æ•°å­—æ˜¾ç¤º
                table_data['ç‚¹å‡»é‡(PV)'] = table_data['ç‚¹å‡»é‡(PV)'].apply(lambda x: f"{int(x):,}")
                table_data['è®¿å®¢æ•°(UV)'] = table_data['è®¿å®¢æ•°(UV)'].apply(lambda x: f"{int(x):,}")
                table_data['æµè§ˆé‡'] = table_data['æµè§ˆé‡'].apply(lambda x: f"{int(x):,}")

                # æ˜¾ç¤ºè¡¨æ ¼
                st.dataframe(
                    table_data,
                    use_container_width=True,
                    hide_index=True
                )

                # æ·»åŠ è¡¨æ ¼è¯´æ˜
                st.markdown("""
                <div style='background-color: #f8f9fa; padding: 10px; border-radius: 5px; margin-top: 10px;'>
                <small>
                ğŸ“ <strong>æ•°æ®è¯´æ˜ï¼š</strong><br>
                â€¢ ç‚¹å‡»é‡(PV)ï¼šåŸºäºä¸åŒ session_id ç»Ÿè®¡çš„æ¯æ—¥ç‚¹å‡»æ¬¡æ•°<br>
                â€¢ è®¿å®¢æ•°(UV)ï¼šåŸºäºä¸åŒ visitor_id ç»Ÿè®¡çš„æ¯æ—¥ç‹¬ç«‹è®¿å®¢æ•°<br>
                â€¢ æµè§ˆé‡(View)ï¼šåŸºäº view_diff å­—æ®µè®¡ç®—çš„æ¯æ—¥æµè§ˆé‡å¢é‡<br>
                â€¢ å›¾è¡¨æ˜¾ç¤ºï¼šä¸‰æ¡æŠ˜çº¿åœ¨åŒä¸€åæ ‡ç³»ä¸­å¯¹æ¯”ï¼Œä¾¿äºè§‚å¯Ÿè¶‹åŠ¿å…³ç³»<br>
                â€¢ æ•°æ®èŒƒå›´ï¼šå½“å‰ç­›é€‰çš„æ—¥æœŸèŒƒå›´
                </small>
                </div>
                """, unsafe_allow_html=True)

            # æµè§ˆé‡å›¾è¡¨
            if "æµè§ˆé‡" in chart_options:
                st.markdown("#### ğŸ“Š æ¯æ—¥æµè§ˆé‡è¶‹åŠ¿")
                views_chart = alt.Chart(data).mark_line(
                    color='#2ca02c', point=True, strokeWidth=2
                ).encode(
                    x=alt.X('date:T', title='æ—¥æœŸ'),
                    y=alt.Y('daily_views:Q', title='æ¯æ—¥æµè§ˆé‡',
                           scale=alt.Scale(domain=[0, data['daily_views'].max() * 1.1]),
                           axis=alt.Axis(format=',')),
                    tooltip=[
                        alt.Tooltip('date:T', title='æ—¥æœŸ', format='%Y-%m-%d'),
                        alt.Tooltip('daily_views:Q', title='æµè§ˆé‡', format=',.0f')
                    ]
                ).properties(
                    title=f"ğŸ“Š {link_url} - æ¯æ—¥æµè§ˆé‡è¶‹åŠ¿",
                    height=300
                )

                st.altair_chart(views_chart, use_container_width=True)

            # è½¬åŒ–ç‡å›¾è¡¨
            if "PVè½¬åŒ–ç‡" in chart_options or "UVè½¬åŒ–ç‡" in chart_options:
                st.markdown("#### ğŸ“Š è½¬åŒ–ç‡è¶‹åŠ¿")

                # å‡†å¤‡è½¬åŒ–ç‡æ•°æ®
                conversion_data = data[['date', 'daily_pv_conversion_rate', 'daily_uv_conversion_rate']].copy()
                conversion_data = conversion_data.melt(
                    id_vars=['date'],
                    value_vars=['daily_pv_conversion_rate', 'daily_uv_conversion_rate'],
                    var_name='type',
                    value_name='rate'
                )
                conversion_data['type'] = conversion_data['type'].map({
                    'daily_pv_conversion_rate': 'PVè½¬åŒ–ç‡',
                    'daily_uv_conversion_rate': 'UVè½¬åŒ–ç‡'
                })

                # ç­›é€‰é€‰ä¸­çš„è½¬åŒ–ç‡ç±»å‹
                if "PVè½¬åŒ–ç‡" in chart_options and "UVè½¬åŒ–ç‡" in chart_options:
                    filtered_conversion_data = conversion_data
                elif "PVè½¬åŒ–ç‡" in chart_options:
                    filtered_conversion_data = conversion_data[conversion_data['type'] == 'PVè½¬åŒ–ç‡']
                else:
                    filtered_conversion_data = conversion_data[conversion_data['type'] == 'UVè½¬åŒ–ç‡']

                conversion_chart = alt.Chart(filtered_conversion_data).mark_line(
                    point=True, strokeWidth=2
                ).encode(
                    x=alt.X('date:T', title='æ—¥æœŸ'),
                    y=alt.Y('rate:Q', title='è½¬åŒ–ç‡ (%)',
                           scale=alt.Scale(domain=[0, filtered_conversion_data['rate'].max() * 1.1])),
                    color=alt.Color('type:N', title='è½¬åŒ–ç‡ç±»å‹'),
                    tooltip=[
                        alt.Tooltip('date:T', title='æ—¥æœŸ', format='%Y-%m-%d'),
                        alt.Tooltip('rate:Q', title='è½¬åŒ–ç‡', format='.2f'),
                        alt.Tooltip('type:N', title='ç±»å‹')
                    ]
                ).properties(
                    title=f"ğŸ“Š {link_url} - è½¬åŒ–ç‡è¶‹åŠ¿",
                    height=300
                )

                st.altair_chart(conversion_chart, use_container_width=True)

            # æ˜¾ç¤ºæ•°æ®è¡¨æ ¼
            with st.expander(f"ğŸ“‹ {link_url} è¯¦ç»†æ•°æ®"):
                # é‡å‘½ååˆ—ç”¨äºæ˜¾ç¤º
                display_data = data.copy()
                display_data = display_data.rename(columns={
                    'daily_clicks': 'æ¯æ—¥ç‚¹å‡»é‡(PV)',
                    'daily_visitors': 'æ¯æ—¥è®¿å®¢æ•°(UV)',
                    'daily_views': 'æ¯æ—¥æµè§ˆé‡',
                    'daily_pv_conversion_rate': 'PVè½¬åŒ–ç‡(%)',
                    'daily_uv_conversion_rate': 'UVè½¬åŒ–ç‡(%)'
                })

                st.dataframe(display_data, use_container_width=True)

                # ä¸‹è½½æŒ‰é’®
                csv_data = display_data.to_csv(index=False)
                st.download_button(
                    label=f"ğŸ“¥ ä¸‹è½½ {link_url} æ•°æ®",
                    data=csv_data,
                    file_name=f"link_conversion_{link_url.replace('https://', '').replace('/', '_')}_{datetime.now().strftime('%Y%m%d')}.csv",
                    mime="text/csv"
                )

            st.markdown("---")  # åˆ†éš”çº¿
        else:
            st.warning(f"é“¾æ¥ {link_url} æš‚æ— æ•°æ®")
    else:
        st.warning("æš‚æ— ç‚¹å‡»æ•°æ®æˆ–è´¦å·æ•°æ®")

with tab6:
    st.markdown("### ğŸ“‹ æ•°æ®è¯¦æƒ…")

    # æ•°æ®ç»Ÿè®¡ä¿¡æ¯
    if summary:
        st.markdown("#### ğŸ“Š æ•°æ®ç»Ÿè®¡")
        summary_html = EnhancedVisualization().create_summary_cards(summary)
        st.markdown(summary_html, unsafe_allow_html=True)

    # åŸå§‹æ•°æ®é¢„è§ˆ
    if processor.merged_df is not None:
        st.markdown("#### ğŸ“„ åˆå¹¶æ•°æ®é¢„è§ˆ")
        st.dataframe(processor.merged_df.head(100), use_container_width=True)

        # æ•°æ®ä¸‹è½½
        csv = processor.merged_df.to_csv(index=False)
        st.download_button(
            label="ğŸ“¥ ä¸‹è½½åˆå¹¶æ•°æ®",
            data=csv,
            file_name=f"enhanced_tiktok_data_{datetime.now().strftime('%Y%m%d')}.csv",
            mime="text/csv"
        )

    # ç‚¹å‡»æ•°æ®é¢„è§ˆ
    if processor.clicks_df is not None:
        st.markdown("#### ğŸ“„ ç‚¹å‡»æ•°æ®é¢„è§ˆ")
        st.dataframe(processor.clicks_df.head(100), use_container_width=True)

        # ç‚¹å‡»æ•°æ®ä¸‹è½½
        clicks_csv = processor.clicks_df.to_csv(index=False)
        st.download_button(
            label="ğŸ“¥ ä¸‹è½½ç‚¹å‡»æ•°æ®",
            data=clicks_csv,
            file_name=f"clicks_data_{datetime.now().strftime('%Y%m%d')}.csv",
            mime="text/csv"
        )

# ç§»é™¤å¯¹æœªå®šä¹‰main()å‡½æ•°çš„è°ƒç”¨
# if __name__ == "__main__":
#     main() 